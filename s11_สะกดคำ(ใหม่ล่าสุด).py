# -*- coding: utf-8 -*-
"""S11_‡∏™‡∏∞‡∏Å‡∏î‡∏Ñ‡∏≥(‡πÉ‡∏´‡∏°‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/169z2UZ3eEe1SQRf7otd55UVY44C-bu1m

# ‡∏Ñ‡∏≥‡∏ó‡∏±‡∏ö‡∏®‡∏±‡∏û‡∏ó‡πå
"""

import pandas as pd
import json

# ----------------------
# ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ó‡∏±‡∏ö‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
# ----------------------
# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö (thai_word, transliteration, category)

loanwords_list = [
    ("‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå", "computer"),
    ("‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï", "internet"),
    ("‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå", "software"),
    ("‡∏Æ‡∏≤‡∏£‡πå‡∏î‡πÅ‡∏ß‡∏£‡πå", "hardware"),
    ("‡∏™‡∏°‡∏≤‡∏£‡πå‡∏ó‡πÇ‡∏ü‡∏ô", "smartphone"),
    ("‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô", "application"),
    ("‡∏≠‡∏µ‡πÄ‡∏°‡∏•", "email"),
    ("‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå", "server"),
    ("‡∏ö‡∏£‡∏≤‡∏ß‡πÄ‡∏ã‡∏≠‡∏£‡πå", "browser"),
    ("‡∏î‡∏≤‡∏ï‡πâ‡∏≤‡πÄ‡∏ö‡∏™", "database"),
    ("‡πÄ‡∏ü‡∏ã‡∏ö‡∏∏‡πä‡∏Å", "facebook"),
    ("‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠","video"),
    ("‡∏≠‡∏¥‡∏ô‡∏™‡∏ï‡∏≤‡πÅ‡∏Å‡∏£‡∏°","instagram"),
    ("‡πÑ‡∏ü‡∏•‡πå","file"),
    ("‡πÅ‡∏ä‡∏ï","chat"),
    ("‡πÅ‡∏ä‡πá‡∏ï","chat"),
    ("‡∏¢‡∏π‡∏ó‡∏π‡∏ö","youtube"),
    ("‡πÇ‡∏ã‡πÄ‡∏ä‡∏µ‡∏¢‡∏•","social"),
    ("‡∏Å‡∏π‡πÄ‡∏Å‡∏¥‡∏•","google"),
    ("‡∏ó‡∏ß‡∏¥‡∏ï‡πÄ‡∏ï‡∏≠‡∏£‡πå","twitter"),
    ("‡∏ä‡πâ‡∏≠‡∏õ‡∏õ‡∏µ‡πâ","shopee"),
    ("‡∏ï‡∏¥‡πä‡∏Å‡∏ï‡πä‡∏≠‡∏Å","tiktok"),
    ("‡πÑ‡∏•‡∏ô‡πå","line"),
    ("‡πÇ‡∏ã‡πÄ‡∏ä‡∏µ‡∏¢‡∏•‡∏°‡∏µ‡πÄ‡∏î‡∏µ‡∏¢","social media"),
    ("‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå","online"), ("‡∏Ñ‡∏≠‡∏ô‡πÄ‡∏ó‡∏ô‡∏ï‡πå","content"), ("‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå","comment"), ("‡πÑ‡∏•‡∏Å‡πå","like"), ("‡πÅ‡∏ä‡∏£‡πå","share"),
    ("‡πÅ‡∏Æ‡∏ä‡πÅ‡∏ó‡πá‡∏Å","hastag"), ("‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ä‡∏±‡∏ô","promotion"), ("‡πÇ‡∏û‡∏£‡πÇ‡∏°‡∏ï","promote"), ("‡πÇ‡∏û‡∏£‡πÑ‡∏ü‡∏•‡πå","profile"), ("‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°","platform"),
    ("‡πÅ‡∏û‡∏•‡πá‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°","platform"), ("‡πÇ‡∏û‡∏™‡∏ï‡πå","post"), ("‡∏≠‡∏¥‡∏ô‡∏ü‡∏•‡∏π‡πÄ‡∏≠‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå","influencer"), ("‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏Ñ‡∏≠‡∏•","video call"),
    ("‡∏•‡∏¥‡∏á‡∏Å‡πå","link"), ("‡πÅ‡∏≠‡πá‡∏Å‡πÄ‡∏Ñ‡∏≤‡∏ô‡∏ï‡πå","account"), ("‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå","feature"), ("‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï","update"), ("‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô","version"),
    ("‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å","notebook"), ("‡πÅ‡∏ó‡πá‡∏ö‡πÄ‡∏•‡πá‡∏ï","tablet"), ("‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•","digital"),("‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°","program"),("‡πÑ‡∏•‡∏ü‡πå","live"),("‡πÄ‡∏Å‡∏°","game"),
    ("‡πÅ‡∏≠‡∏õ","app"),("‡πÅ‡∏≠‡πá‡∏õ","app"),("‡∏Å‡∏≤‡∏£‡πå‡∏ï‡∏π‡∏ô","cartoon"),("‡∏Ñ‡∏≠‡∏•‡πÄ‡∏ã‡∏ô‡πÄ‡∏ï‡∏≠‡∏£‡πå","call center"),
]

# ----------------------
# ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô DataFrame
# ----------------------
df = pd.DataFrame(loanwords_list, columns=["thai_word", "transliteration"])

# ----------------------
# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô CSV
# ----------------------
csv_path = "thai_loanwords_new_update.csv"
df.to_csv(csv_path, index=False, encoding="utf-8-sig")

# ----------------------
# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô JSON
# ----------------------
json_path = "thai_loanwords_new_update(1).json"
with open(json_path, "w", encoding="utf-8") as f:
    json.dump(df.to_dict(orient="records"), f, ensure_ascii=False, indent=2)

print(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß:\n- {csv_path}\n- {json_path}")

"""# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î‡πÅ‡∏ï‡πà pythainlp ‡∏ï‡∏£‡∏ß‡∏à‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠"""

import pandas as pd
import json

# ----------------------
# ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ó‡∏±‡∏ö‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
# ----------------------
# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö (thai_word, transliteration, category)

common_misspellings = [
  {"wrong": "‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏∏", "right": "‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï"},
  {"wrong": "‡∏†‡∏π‡∏°‡∏π‡∏¥‡∏Ñ‡∏∏‡πâ‡∏°‡∏Å‡∏±‡∏ô", "right": "‡∏†‡∏π‡∏°‡∏¥‡∏Ñ‡∏∏‡πâ‡∏°‡∏Å‡∏±‡∏ô"},
  {"wrong": "‡∏Å‡∏£‡∏∞‡∏à‡πà‡∏≤‡∏¢", "right": "‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢"},
  {"wrong": "‡πÅ‡∏Å‡πà‡∏õ‡∏±‡∏ç‡∏´‡∏≤", "right": "‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤"},
  {"wrong": "‡πÄ‡∏à‡∏î‡∏ô‡∏≤", "right": "‡πÄ‡∏à‡∏ï‡∏ô‡∏≤"},
  {"wrong": "‡∏´‡∏£‡∏¥‡∏≠", "right": "‡∏´‡∏£‡∏∑‡∏≠"},
  {"wrong": "‡∏™‡πà‡∏≠‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå", "right": "‡∏™‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå"},
  {"wrong": "‡∏ß‡∏¥‡∏û‡∏≤‡∏Å‡∏ß‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡πå", "right": "‡∏ß‡∏¥‡∏û‡∏≤‡∏Å‡∏©‡πå‡∏ß‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡πå"},
  {"wrong": "‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏•‡∏°", "right": "‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°"},
  {"wrong": "‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πå", "right": "‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå"},
  {"wrong": "‡∏™‡∏±‡∏ì‡∏ç‡∏≤‡∏ì", "right": "‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì"},
  {"wrong": "‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ç", "right": "‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì"},
  {"wrong": "‡∏™‡∏±‡∏ì‡∏ç‡∏≤‡∏ç", "right": "‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì"},
  {"wrong": "‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πå", "right": "‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•"},
  {"wrong": "‡∏•‡∏≥‡∏ö‡∏≤‡∏Å‡πå", "right": "‡∏•‡∏≥‡∏ö‡∏≤‡∏Å"},
  {"wrong": "‡∏ó‡∏±‡∏®‡∏ô‡∏∞‡∏Ñ‡∏ï‡∏¥", "right": "‡∏ó‡∏±‡∏®‡∏ô‡∏Ñ‡∏ï‡∏¥"},
  {"wrong": "‡∏£‡∏±‡∏õ‡∏ó‡∏≤‡∏ô", "right": "‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô"},
  {"wrong": "‡∏®‡∏≤‡∏™‡∏ô‡∏≤‡∏û‡∏∏‡∏ó‡∏ò‡∏ó‡πå", "right": "‡∏®‡∏≤‡∏™‡∏ô‡∏≤‡∏û‡∏∏‡∏ó‡∏ò"},
  {"wrong": "‡∏™‡∏±‡∏û‡πÄ‡∏û", "right": "‡∏™‡∏±‡∏û‡πÄ‡∏û"},
  {"wrong": "‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏π‡πà‡πå", "right": "‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏π‡πà"},
  {"wrong": "‡∏≠‡∏±‡∏ò‡∏¢‡∏≤‡∏™‡∏±‡∏¢‡∏¢‡πå", "right": "‡∏≠‡∏±‡∏ò‡∏¢‡∏≤‡∏®‡∏±‡∏¢"},
  {"wrong": "‡πÄ‡∏ú‡∏≠‡πÄ‡∏£‡πä‡∏≠", "right": "‡πÄ‡∏ú‡∏•‡∏≠‡πÄ‡∏£‡∏≠"},
  {"wrong": "‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏ï‡∏£‡πå", "right": "‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå"},
  {"wrong": "‡∏à‡∏£‡∏¥‡∏ç", "right": "‡πÄ‡∏à‡∏£‡∏¥‡∏ç"},
  {"wrong": "‡πÄ‡∏û‡∏≤‡∏∞‡∏ß‡πà‡∏≤", "right": "‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ß‡πà‡∏≤"},
  {"wrong": "‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡πå", "right": "‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô"},
  {"wrong": "‡∏û‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ô‡πå", "right": "‡∏û‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ô‡∏≤"},
  {"wrong": "‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏ì‡πå", "right": "‡∏™‡∏±‡∏á‡∏Ñ‡∏°"},
  {"wrong": "‡πÉ‡∏™‡πà‡πÉ‡∏à‡πå", "right": "‡πÉ‡∏™‡πà‡πÉ‡∏à"},
  {"wrong": "‡∏™‡∏±‡∏ì‡∏ê‡∏≤‡∏ô", "right": "‡∏™‡∏±‡∏ì‡∏ê‡∏≤‡∏ô"},
  {"wrong": "‡∏Å‡∏è‡∏´‡∏°‡∏≤‡∏¢", "right": "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢"},
  {"wrong": "‡∏ò‡∏∏‡∏£‡∏∞‡∏Å‡∏¥‡∏à", "right": "‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à"},
  {"wrong": "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡∏≤‡∏á", "right": "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á"},
  {"wrong": "‡∏•‡πâ‡∏≠‡πÅ‡∏´‡∏•‡∏°", "right": "‡∏•‡πà‡∏≠‡πÅ‡∏´‡∏•‡∏°"},
  {"wrong": "‡∏´‡∏•‡∏á‡πÑ‡∏´‡∏•", "right": "‡∏´‡∏•‡∏á‡πÉ‡∏´‡∏•"},
  {"wrong": "‡∏ä‡∏∑‡πâ‡∏≠", "right": "‡∏ã‡∏∑‡πâ‡∏≠"} ,
  {"wrong": "‡πÇ‡∏ó‡∏ö", "right": "‡πÇ‡∏ó‡∏©"},
  {"wrong": "‡∏£‡∏±‡∏ö‡∏ú‡∏∑‡∏î‡∏ä‡∏≠‡∏ö", "right": "‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö"},
  {"wrong": "‡πÅ‡∏û‡∏£‡πà‡∏Å‡∏£‡∏∞‡∏à‡πà‡∏≤‡∏¢", "right": "‡πÅ‡∏û‡∏£‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢"},
  {"wrong": "‡∏Ç‡πâ‡∏≠‡∏°‡∏∏‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£", "right": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£"},
  {"wrong": "‡∏Ç‡∏≤‡∏ß‡∏™‡∏≤‡∏£", "right": "‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£"},
  {"wrong": "‡∏™‡πç‡∏≤‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏°", "right": "‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏°‡∏û‡πå"},
  {"wrong": "‡∏Ñ‡∏∏‡∏ô‡∏†‡∏≤‡∏û", "right": "‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û"},
  {"wrong": "‡∏£‡∏∞‡∏°‡∏±‡∏ï‡∏£‡∏∞‡∏ß‡∏±‡∏á", "right": "‡∏£‡∏∞‡∏°‡∏±‡∏î‡∏£‡∏∞‡∏ß‡∏±‡∏á"},
  {"wrong": "‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏ô‡∏ô‡∏±‡∏ô", "right": "‡∏Å‡∏≤‡∏£‡∏û‡∏ô‡∏±‡∏ô"},
  {"wrong": "‡∏Å‡πâ‡∏≤‡∏ß‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏ï", "right": "‡∏Å‡πâ‡∏≤‡∏ß‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î"},
  {"wrong": "‡∏≠‡∏≤‡∏ä‡∏¥‡∏û", "right": "‡∏≠‡∏≤‡∏ä‡∏µ‡∏û"},
  {"wrong": "‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏ä‡∏ß‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏∞", "right": "‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏ä‡∏ß‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠"},
  {"wrong": "‡πÅ‡∏≠‡∏ö‡πÅ‡∏ü‡∏á", "right": "‡πÅ‡∏≠‡∏ö‡πÅ‡∏ù‡∏á"},
  {"wrong": "‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏î", "right": "‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå"},
  {"wrong": "‡πÄ‡∏™‡∏∑‡πà‡∏≠‡∏°‡πÄ‡∏™‡∏¢", "right": "‡πÄ‡∏™‡∏∑‡πà‡∏≠‡∏°‡πÄ‡∏™‡∏µ‡∏¢"},
  {"wrong": "‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡πà‡∏≠", "right": "‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠"},
  {"wrong": "‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏ï‡∏£‡πå", "right": "‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå"},
  {"wrong": "‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πå", "right": "‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå"},
  {"wrong": "‡πÄ‡∏™‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à", "right": "‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à"},
  {"wrong": "‡πÄ‡∏®‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡∏Å", "right": "‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à"},
  {"wrong": "‡∏õ‡∏£‡∏±‡∏ä‡∏¢‡∏≤", "right": "‡∏õ‡∏£‡∏±‡∏ä‡∏ç‡∏≤"},
  {"wrong": "‡∏°‡∏´‡∏≤‡∏•‡∏±‡∏¢‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢", "right": "‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢"},
  {"wrong": "‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ò‡∏≤‡∏•‡∏±‡∏¢", "right": "‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢"},
  {"wrong": "‡∏≠‡∏∏‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°", "right": "‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°"},
  {"wrong": "‡∏≠‡∏∏‡∏™‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°", "right": "‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°"},
  {"wrong": "‡∏û‡∏±‡∏í‡∏ì‡∏≤", "right": "‡∏û‡∏±‡∏í‡∏ô‡∏≤"},
  {"wrong": "‡∏ô‡∏∞‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢", "right": "‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢"},
  {"wrong": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∑‡πâ‡∏ô", "right": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô"},
  {"wrong": "‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ô‡πå", "right": "‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ"},
  {"wrong": "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÑ‡∏à", "right": "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à"},
  {"wrong": "‡∏ó‡πÄ‡∏•‡∏≤‡∏∞", "right": "‡∏ó‡∏∞‡πÄ‡∏•‡∏≤‡∏∞"},
  {"wrong": "‡∏ó‡∏∞‡πÄ‡∏£‡∏≤‡∏∞", "right": "‡∏ó‡∏∞‡πÄ‡∏•‡∏≤‡∏∞"},
  {"wrong": "‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢‡πå", "right": "‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢"},
  {"wrong": "‡∏≠‡∏≤‡∏£‡∏°", "right": "‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå"},
  {"wrong": "‡∏õ‡∏±‡∏ç‡∏¢‡∏≤", "right": "‡∏õ‡∏±‡∏ç‡∏ç‡∏≤"},
  {"wrong": "‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ç‡∏ô‡πå", "right": "‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå"},
  {"wrong": "‡πÄ‡∏°‡∏¥‡∏≠‡∏á", "right": "‡πÄ‡∏°‡∏∑‡∏≠‡∏á"},
  {"wrong": "‡∏Ç‡πâ‡∏≠‡∏à‡∏≤‡∏Å‡∏±‡∏Å", "right": "‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î"},
  {"wrong": "‡πÄ‡∏£‡∏¥‡∏≠‡∏á", "right": "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á"},
  {"wrong": "‡πÅ‡∏£‡∏∞", "right": "‡πÅ‡∏•‡∏∞"},
  {"wrong": "‡∏î‡πâ‡∏≤‡∏¢", "right": "‡∏î‡πâ‡∏ß‡∏¢"},
  {"wrong": "‡∏™‡∏¥‡∏≠", "right": "‡∏™‡∏∑‡πà‡∏≠"},{"wrong": "‡∏™‡∏¥‡∏á", "right": "‡∏™‡∏¥‡πà‡∏á"},
  {"wrong": "‡πÄ‡∏™‡πà‡∏≠‡∏°‡πÄ‡∏™‡∏µ‡∏¢", "right": "‡πÄ‡∏™‡∏∑‡πà‡∏≠‡∏°‡πÄ‡∏™‡∏µ‡∏¢"},{"wrong": "‡∏´‡∏∑‡∏£‡∏∑‡∏≠", "right": "‡∏´‡∏£‡∏∑‡∏≠"},
  {"wrong": "‡∏á‡∏≤‡∏ô‡πÄ‡∏Ç‡∏µ‡∏¢‡∏á", "right": "‡∏á‡∏≤‡∏ô‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô"},{"wrong": "‡∏à‡∏¥‡∏ï‡∏ô‡∏≤", "right": "‡πÄ‡∏à‡∏ï‡∏ô‡∏≤"},
  {"wrong": "‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠", "right": "‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠"},{"wrong": "‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏•‡∏±‡∏ß", "right": "‡∏ô‡πà‡∏≤‡∏Å‡∏•‡∏±‡∏ß"},
  {"wrong": "‡∏°‡∏¥‡∏ï‡∏£‡∏â‡∏≤‡∏ä‡∏µ‡∏û", "right": "‡∏°‡∏¥‡∏à‡∏â‡∏≤‡∏ä‡∏µ‡∏û"},{"wrong": "‡∏ó‡∏∂‡∏á", "right": "‡∏ñ‡∏∂‡∏á"},
  {"wrong": "‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ç‡∏ô‡πå", "right": "‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå"},{"wrong": "‡πÅ‡∏™‡∏≠‡∏á", "right": "‡πÅ‡∏™‡∏ß‡∏á"},
  {"wrong": "‡∏Ç‡πà‡∏ß‡∏ß", "right": "‡∏Ç‡πà‡∏≤‡∏ß"},{"wrong": "‡∏°‡∏±‡∏ô‡πÉ‡∏à", "right": "‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à"},{"wrong": "‡∏õ‡∏£‡πÇ‡∏¢‡∏ä‡∏ô‡πå", "right": "‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå"},
  {"wrong": "‡∏Ç‡πà‡∏≤‡∏â‡∏™‡∏≤‡∏£", "right": "‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£"},{"wrong": "‡πÅ‡∏ü‡∏á", "right": "‡πÅ‡∏ù‡∏á"},{"wrong": "‡∏™‡∏î‡∏ß‡∏Å", "right": "‡∏™‡∏∞‡∏î‡∏ß‡∏Å"},
  {"wrong": "‡∏î‡πâ‡∏¢", "right": "‡∏î‡πâ‡∏ß‡∏¢"},{"wrong": "‡πÇ‡∏Ñ‡πÄ‡∏ã‡πá‡∏ô‡πÄ‡∏ï‡∏≠‡∏£‡πå", "right": "‡∏Ñ‡∏≠‡∏•‡πÄ‡∏ã‡∏ô‡πÄ‡∏ï‡∏≠‡∏£‡πå"},{"wrong": "‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏¢", "right": "‡πÄ‡∏´‡∏¢‡∏∑‡πà‡∏≠"},
  {"wrong": "‡∏°‡∏µ‡∏à‡∏≤‡∏ä‡∏µ‡∏û", "right": "‡∏°‡∏¥‡∏à‡∏â‡∏≤‡∏ä‡∏µ‡∏û"},{"wrong": "‡πÅ‡∏≠‡∏û‡∏û‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡πâ‡∏ô", "right": "‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô"},
  {"wrong": "‡πÇ‡∏ó‡∏£‡∏ó‡∏±‡∏™", "right": "‡πÇ‡∏ó‡∏£‡∏ó‡∏±‡∏®‡∏ô‡πå"},{"wrong": "‡∏™‡∏∞‡∏ö‡∏≤‡∏¢", "right": "‡∏™‡∏ö‡∏≤‡∏¢"},
  {"wrong": "‡∏à‡∏¥‡πã‡∏Å‡πÇ‡∏Å‡πã", "right": "‡∏à‡∏¥‡πä‡∏Å‡πÇ‡∏Å‡πã"},{"wrong": "‡∏Ñ‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï", "right": "‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï"},{"wrong": "‡∏°‡∏¥‡∏â‡∏â‡∏≤‡∏ä‡∏µ‡∏û", "right": "‡∏°‡∏¥‡∏à‡∏â‡∏≤‡∏ä‡∏µ‡∏û"},
  {"wrong": "‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πâ‡∏≠", "right": "‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠"},{"wrong": "‡∏°‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û", "right": "‡∏°‡∏¥‡∏à‡∏â‡∏≤‡∏ä‡∏µ‡∏û"},
  {"wrong": "‡∏°‡∏¥‡∏ß‡∏ä‡∏≤‡∏ä‡∏µ‡∏û", "right": "‡∏°‡∏¥‡∏à‡∏â‡∏≤‡∏ä‡∏µ‡∏û"},{"wrong": "‡∏Ç‡∏≥‡πÄ‡∏õ‡πá‡∏ô", "right": "‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"},
  {"wrong": "‡∏™‡∏¥‡πà‡∏ô‡∏Ñ‡πâ‡∏≤", "right": "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤"},{"wrong": "‡πÄ‡∏ó‡∏µ‡∏à", "right": "‡πÄ‡∏ó‡πá‡∏à"},{"wrong": "‡πÅ‡∏û‡πà", "right": "‡πÅ‡∏û‡∏£‡πà"},
  {"wrong": "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏¥‡∏Å", "right": "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏±‡∏Å‡∏£"},{"wrong": "‡πÇ‡∏Ü‡∏ì‡∏≤", "right": "‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤"},
  {"wrong": "‡πÄ‡∏´‡∏¢‡∏∑‡∏≠", "right": "‡πÄ‡∏´‡∏¢‡∏∑‡πà‡∏≠"},{"wrong": "‡∏Å‡∏£‡∏∞‡πÅ‡∏™‡πà", "right": "‡∏Å‡∏£‡∏∞‡πÅ‡∏™"},{"wrong": "‡∏Ñ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "right": "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"},
  {"wrong": "‡∏™‡∏£‡πà‡∏≤‡∏á", "right": "‡∏™‡∏£‡πâ‡∏≤‡∏á"},{"wrong": "‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏¢‡πå", "right": "‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå"},
  {"wrong": "‡πÇ‡∏ã‡∏©‡∏ì‡∏≤", "right": "‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤"},{"wrong": "‡πÄ‡∏´‡∏¢‡∏∑‡πâ‡∏≠", "right": "‡πÄ‡∏´‡∏¢‡∏∑‡πà‡∏≠"},{"wrong": "‡∏™‡∏∞‡∏ö‡∏≤‡∏¢", "‡πÄ‡∏£‡∏µ‡∏ß": "‡πÄ‡∏£‡πá‡∏ß"},
  {"wrong": "‡πÄ‡∏û‡∏≠‡πà", "right": "‡πÄ‡∏û‡∏∑‡πà‡∏≠"},{"wrong": "‡∏à‡∏î‡∏±", "right": "‡∏à‡∏±‡∏î"},{"wrong": "‡πÄ‡∏´‡∏¢‡∏≠‡∏∑‡πà", "right": "‡πÄ‡∏´‡∏¢‡∏∑‡πà‡∏≠"},
  {"wrong": "‡πÇ‡∏Ñ‡∏•‡∏≠‡πÄ‡∏ã‡πá‡∏ô‡πÄ‡∏ï‡∏≠‡∏£‡πå", "right": "‡∏Ñ‡∏≠‡∏•‡πÄ‡∏ã‡∏ô‡πÄ‡∏ï‡∏≠‡∏£‡πå"},{"wrong": "‡∏â‡∏≤‡∏à", "right": "‡∏≠‡∏≤‡∏à"},
  {"wrong": "‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡πà‡∏≤‡∏ß", "right": "‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤"},{"wrong": "‡πÄ‡∏ó‡πà‡∏≤‡∏ñ‡∏±‡∏ö", "right": "‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö"},
  {"wrong": "‡∏≠‡∏¢‡πà‡∏≤‡∏Å", "right": "‡∏≠‡∏¢‡∏≤‡∏Å"},{"wrong": "‡πÇ‡∏û‡∏ä", "right": "‡πÇ‡∏û‡∏™‡∏ï‡πå"},
  {"wrong": "‡πÄ‡∏Å‡∏™‡πå", "right": "‡πÄ‡∏Å‡∏°"},{"wrong": "‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö", "right": "‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô"},{"wrong": "‡∏°‡∏¥‡∏â‡∏≤‡∏ä‡∏µ‡∏û", "right": "‡∏°‡∏¥‡∏à‡∏â‡∏≤‡∏ä‡∏µ‡∏û"},
  {"wrong": "‡πÄ‡∏î‡∏∑‡∏≠‡∏â", "right": "‡πÄ‡∏î‡∏∑‡∏≠‡∏î"},{"wrong": "‡πÅ‡∏ß‡∏õ", "right": "‡πÄ‡∏ß‡πá‡∏ö"},{"wrong": "‡∏Ç‡∏≠‡πâ", "right": "‡∏Ç‡πâ‡∏≠"},
  {"wrong": "‡∏ß‡∏¥‡∏û‡∏≤‡∏Å‡∏û‡πå‡∏ß‡∏¥‡∏à‡∏≤‡∏£", "right": "‡∏ß‡∏¥‡∏û‡∏≤‡∏Å‡∏©‡πå‡∏ß‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡πå"},{"wrong": "‡∏´‡∏•‡∏≠‡∏Å‡∏•‡πà‡∏ß‡∏á", "right": "‡∏´‡∏•‡∏≠‡∏Å‡∏•‡∏ß‡∏á"},
  {"wrong": "‡πÄ‡∏õ‡∏•‡∏∑‡πà‡∏≠‡∏¢", "right": "‡πÄ‡∏õ‡∏•‡∏∑‡∏≠‡∏¢"},{"wrong": "‡πÄ‡∏¢‡∏≠‡∏±", "right": "‡πÄ‡∏¢‡∏≠‡∏∞"},{"wrong": "‡∏•‡πâ‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤", "right": "‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤"},{"wrong": "‡∏´‡∏£‡∏±‡∏™", "right": "‡∏£‡∏´‡∏±‡∏™"},
  {"wrong": "‡∏¢‡∏∏‡∏Ñ‡∏™‡∏°‡∏¥‡∏±‡∏¢", "right": "‡∏¢‡∏∏‡∏Ñ‡∏™‡∏°‡∏±‡∏¢"},{"wrong": "‡∏ú‡∏π‡πâ‡∏ñ‡∏∂‡∏á", "right": "‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á"},{"wrong": "‡∏°‡∏¥‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå", "right": "‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå"},
  {"wrong": "‡πÄ‡∏õ‡∏¥‡∏ô", "right": "‡πÄ‡∏õ‡πá‡∏ô"},{"wrong": "‡πÄ‡∏û‡∏£‡∏≤‡∏â‡∏∞‡∏ô‡∏±‡πâ‡∏ô", "right": "‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏â‡∏∞‡∏ô‡∏±‡πâ‡∏ô"},{"wrong": "‡∏Ñ‡πâ‡∏≤‡∏Ç‡∏≤‡∏ö", "right": "‡∏Ñ‡πâ‡∏≤‡∏Ç‡∏≤‡∏¢"},
  {"wrong": "‡πÄ‡∏ä‡∏∑‡πà‡∏•‡∏ü‡∏±‡∏á", "right": "‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏±‡∏á"},{"wrong": "‡∏£‡∏π‡∏ö‡πÅ‡∏ö‡∏ö", "right": "‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö"},{"wrong": "‡πÄ‡∏û‡∏™", "right": "‡πÄ‡∏û‡∏®"},{"wrong": "‡πÅ‡∏ä‡∏ó", "right": "‡πÅ‡∏ä‡∏ï"},
  {"wrong": "‡∏Å‡πâ‡πÄ‡∏•‡πà‡∏ô", "right": "‡∏Å‡πá‡πÄ‡∏•‡πà‡∏ô"},{"wrong": "‡∏Å‡πâ‡∏à‡∏∞", "right": "‡∏Å‡πá‡∏à‡∏∞"},{"wrong": "‡∏Ç‡πâ‡∏ß‡∏°‡∏π‡∏•", "right": "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"},{"wrong": "‡πÄ‡∏õ‡πâ‡∏ô", "right": "‡πÄ‡∏õ‡πá‡∏ô"},
  {"wrong": "‡πÅ‡∏™‡∏ß‡∏á‡∏ó‡∏≤", "right": "‡πÅ‡∏™‡∏ß‡∏á‡∏´‡∏≤"},{"wrong": "‡∏Å‡∏±‡∏á‡∏ß‡∏ô", "right": "‡∏Å‡∏±‡∏á‡∏ß‡∏•"},{"wrong": "‡πÄ‡∏£‡∏∑‡∏≠‡∏á", "right": "‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á"},
  {"wrong": "‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏î", "right": "‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ"},{"wrong": "‡∏Ñ‡∏≠‡∏•‡πÄ‡∏ã‡πá‡∏ô‡πÄ‡∏ï‡∏≠‡∏£‡πå", "right": "‡∏Ñ‡∏≠‡∏•‡πÄ‡∏ã‡∏ô‡πÄ‡∏ï‡∏≠‡∏£‡πå"},{"wrong": "‡∏™‡∏±‡∏ß‡∏Ñ‡∏°", "right": "‡∏™‡∏±‡∏á‡∏Ñ‡∏°"},
  {"wrong": "‡∏™‡∏¥‡πà‡∏≠", "right": "‡∏™‡∏∑‡πà‡∏≠"},{"wrong": "‡∏™‡∏∑‡πà‡∏•", "right": "‡∏™‡∏∑‡πà‡∏≠"},{"wrong": "‡∏£‡∏ß‡∏î‡πÄ‡∏£‡∏¥‡∏ß", "right": "‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß"},{"wrong": "‡πÄ‡∏™‡∏∑‡πà‡∏≠‡∏°‡πÄ‡∏•‡∏µ‡∏¢", "right": "‡πÄ‡∏™‡∏∑‡πà‡∏≠‡∏°‡πÄ‡∏™‡∏µ‡∏¢"},
  {"wrong": "‡∏™‡∏±‡∏Ñ‡∏°", "right": "‡∏™‡∏±‡∏á‡∏Ñ‡∏°"},{"wrong": "‡∏Å‡∏•‡∏∏‡∏°‡∏Ñ‡∏ô", "right": "‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ñ‡∏ô"},{"wrong": "‡∏ó‡∏µ‡πà‡∏°‡∏¥", "right": "‡∏ó‡∏µ‡πà‡∏°‡∏µ"},
  {"wrong": "‡∏û‡∏•‡∏î‡∏µ", "right": "‡∏ú‡∏•‡∏î‡∏µ"},{"wrong": "‡πÄ‡∏´‡∏¢‡∏µ‡πà‡∏≠", "right": "‡πÄ‡∏´‡∏¢‡∏∑‡πà‡∏≠"},{"wrong": "‡πÉ‡∏ä‡πà‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á", "right": "‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á"},
  {"wrong": "‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏¥", "right": "‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï"},{"wrong": "‡πÄ‡∏Ç‡πà‡∏≤‡∏ñ‡∏∂‡∏á", "right": "‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á"},{"wrong": "‡πÄ‡∏´‡∏¢‡∏¥‡πà‡∏≠", "right": "‡πÄ‡∏´‡∏¢‡∏∑‡πà‡∏≠"},
  {"wrong": "‡∏ï‡∏•‡∏≤‡∏Ñ", "right": "‡∏ï‡∏•‡∏≤‡∏î"},{"wrong": "‡∏™‡∏∑‡πà‡∏¢‡∏´‡∏≤‡∏¢", "right": "‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤"},{"wrong": "‡∏®‡∏¥‡∏•‡∏ò‡∏£‡∏£‡∏°", "right": "‡∏®‡∏µ‡∏•‡∏ò‡∏£‡∏£‡∏°"},
  {"wrong": "‡∏û‡∏∂‡∏á", "right": "‡∏û‡∏∂‡πà‡∏á"},{"wrong": "‡∏ô‡∏≠‡∏Å‡∏á‡∏≤‡∏Å", "right": "‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å"},{"wrong": "‡∏ô‡∏¢‡∏°", "right": "‡∏ô‡∏¥‡∏¢‡∏°"},{"wrong": "‡∏´‡∏ß‡∏±‡∏ô‡πá‡∏≠‡∏Å", "right": "‡∏´‡∏±‡∏ß‡∏ô‡πá‡∏≠‡∏Å"},
  {"wrong": "‡∏Ñ‡∏á‡∏≤‡∏°", "right": "‡∏Ñ‡∏ß‡∏≤‡∏°"},{"wrong": "‡∏Ñ‡∏ì‡∏∏‡∏ò‡∏£‡∏£‡∏°", "right": "‡∏Ñ‡∏∏‡∏ì‡∏ò‡∏£‡∏£‡∏°"},{"wrong": "‡πÄ‡∏°‡∏∑‡πâ‡∏≠", "right": "‡πÄ‡∏°‡∏∑‡πà‡∏≠"},{"wrong": "‡∏´‡∏£‡∏µ‡∏≠", "right": "‡∏´‡∏£‡∏∑‡∏≠"},
  {"wrong": "‡∏™‡∏≤‡∏£‡∏°‡∏≤‡∏£‡∏ñ", "right": "‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ"},{"wrong": "‡∏Ç‡πà‡∏≠‡∏î‡∏µ", "right": "‡∏Ç‡πâ‡∏≠‡∏î‡∏µ"},{"wrong": "‡πÇ‡∏ó‡∏£‡∏™‡∏±‡∏û", "right": "‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå"},{"wrong": "‡πÉ‡∏ä‡πâ‡∏™‡∏∑‡πâ‡∏≠", "right": "‡πÉ‡∏ä‡πâ‡∏™‡∏∑‡πà‡∏≠"},
  {"wrong": "‡πÉ‡∏Ç‡πâ", "right": "‡πÉ‡∏´‡πâ"},{"wrong": "‡πÉ‡∏Ç‡πâ", "right": "‡πÉ‡∏ä‡πâ"},{"wrong": "‡∏Å‡πá‡∏ï‡∏≤‡∏ô", "right": "‡∏Å‡πá‡∏ï‡∏≤‡∏°"},{"wrong": "‡∏™‡πå‡∏á‡∏Ñ‡∏°", "right": "‡∏™‡∏±‡∏á‡∏Ñ‡∏°"},
  {"wrong": "‡πÄ‡∏™‡πà‡∏¢‡∏´‡∏≤‡∏¢", "right": "‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢"},{"wrong": "‡∏ä‡∏Å‡∏à‡∏∏‡∏á", "right": "‡∏ä‡∏±‡∏Å‡∏à‡∏π‡∏á"},{"wrong": "‡πÄ‡∏ä‡∏ô", "right": "‡πÄ‡∏ä‡πà‡∏ô"},{"wrong": "‡∏™‡∏∑‡∏≠‡∏™‡∏≤‡∏£", "right": "‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£"},
  {"wrong": "‡∏†‡∏π‡∏°‡∏π‡∏¥‡∏Ñ‡∏∏‡πâ‡∏°‡∏Å‡∏±‡∏ô", "right": "‡∏†‡∏π‡∏°‡∏¥‡∏Ñ‡∏∏‡πâ‡∏°‡∏Å‡∏±‡∏ô"},{"wrong": "‡∏ñ‡∏∂‡πà‡∏á", "right": "‡∏ñ‡∏∂‡∏á"},{"wrong": "‡∏£‡∏∞‡∏°‡∏±‡∏ï‡∏£‡∏∞‡∏ß‡∏±‡∏á", "right": "‡∏£‡∏∞‡∏°‡∏±‡∏î‡∏£‡∏∞‡∏ß‡∏±‡∏á"},
  {"wrong": "‡∏ó‡∏¥‡πà", "right": "‡∏ó‡∏µ‡πà"},{"wrong": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏ô", "right": "‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô"},{"wrong": "‡∏™‡∏£‡∏±‡∏≤‡∏á", "right": "‡∏™‡∏£‡πâ‡∏≤‡∏á"},{"wrong": "‡∏Ñ‡∏ß‡∏°", "right": "‡∏Ñ‡∏ß‡∏≤‡∏°"},
  {"wrong": "‡∏™‡∏∑‡πà‡∏ß", "right": "‡∏™‡∏∑‡πà‡∏≠"},{"wrong": "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡∏≤‡∏á", "right": "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á"},{"wrong": "‡∏ä‡∏±‡∏ô‡∏ô‡∏≥", "right": "‡∏ä‡∏±‡πâ‡∏ô‡∏ô‡∏≥"},
]


# ----------------------
# ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô DataFrame
# ----------------------
df = pd.DataFrame(common_misspellings)

# ----------------------
# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô CSV
# ----------------------
csv_path = "update_common_misspellings.csv"
df.to_csv(csv_path, index=False, encoding="utf-8-sig")

# ----------------------
# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô JSON
# ----------------------
json_path = "update_common_misspellings.json"
with open(json_path, "w", encoding="utf-8") as f:
    json.dump(df.to_dict(orient="records"), f, ensure_ascii=False, indent=2)

print(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß:\n- {csv_path}\n- {json_path}")

"""# ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏â‡∏µ‡∏Å‡πÑ‡∏î‡πâ‡πÄ‡∏¢‡∏≠‡∏∞ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏â‡∏µ‡∏Å‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô json ‡πÅ‡∏ó‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°"""

import json

# ‚úÖ ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ
splitable_phrases = [
    "‡πÅ‡∏°‡πâ‡∏ß‡πà‡∏≤",
    "‡∏ñ‡πâ‡∏≤‡πÅ‡∏°‡πâ‡∏ß‡πà‡∏≤",
    "‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤",
    "‡πÅ‡∏ï‡πà‡∏ó‡∏ß‡πà‡∏≤",
    "‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å",
    "‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô",
    "‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏â‡∏∞‡∏ô‡∏±‡πâ‡∏ô",
    "‡∏ï‡∏Å‡πÄ‡∏õ‡πá‡∏ô",
    "‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£",
    "‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå",
    "‡∏ó‡∏≥‡πÑ‡∏î‡πâ",
    "‡∏Å‡πá‡∏ï‡∏≤‡∏°",
    "‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤",
    "‡∏°‡∏µ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö",
    "‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á",
    "‡∏£‡∏π‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏±‡∏ô",
    "‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤‡∏ä‡∏ß‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠",
    "‡∏î‡∏µ‡πÅ‡∏ï‡πà",
    "‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà",
    "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£",
    "‡∏ó‡∏±‡πà‡∏ß‡πÇ‡∏•‡∏Å",
    "‡∏¢‡∏±‡∏á‡∏°‡∏µ",
    "‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î",
    "‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏ó‡∏©",
    "‡πÑ‡∏°‡πà‡∏°‡∏µ",
    "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á",
    "‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô",
    "‡∏ú‡∏¥‡∏î‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢",
    "‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô",
    "‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå",
    "‡∏ó‡∏≥‡∏ú‡∏¥‡∏î‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢",
    "‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ",
    "‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö",
    "‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô",
    "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô",
    "‡∏°‡∏µ‡∏™‡∏ï‡∏¥",
    "‡∏Å‡∏¥‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß",
    "‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ",
    "‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô",
    "‡∏¢‡∏±‡∏á‡∏Ñ‡∏á",
    "‡πÅ‡∏™‡∏ß‡∏á‡∏´‡∏≤‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå",
    "‡∏Å‡∏≤‡∏£‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠",
    "‡∏ä‡∏ß‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠",
    "‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞",
    "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏∑‡πà‡∏≠‡∏°‡πÄ‡∏™‡∏µ‡∏¢",
    "‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á",
    "‡∏ú‡∏π‡πâ‡∏ã‡∏∑‡πâ‡∏≠",
    "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô"
    "‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡∏™‡∏ö‡∏≤‡∏¢"
]

# üîΩ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô JSON
output_path = "/content/drive/MyDrive/splitable_phrases.json"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(splitable_phrases, f, ensure_ascii=False, indent=2)

print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: {output_path}")

"""# ‡∏•‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î+‡∏ô‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ < 50  ‡πÑ‡∏î‡πâ 89/100

https://silpakorn-my.sharepoint.com/:x:/g/personal/singtong_c2_su_ac_th/ES8JuoqVpr1Iiae1R8cR1PABuBwta0dDfNNACKVMiggi1w?e=WFSc3F
"""

import re
import json
import difflib
import requests
from sentence_transformers import SentenceTransformer, util
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus import thai_words
from pythainlp.tag import pos_tag
from pythainlp.util import normalize
import pandas as pd
from pythainlp.spell import spell

# ---- ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏Ñ‡∏≥ ----
# ‡πÇ‡∏´‡∏•‡∏î whitelist ‡∏Ñ‡∏≥‡∏ó‡∏±‡∏ö‡∏®‡∏±‡∏û‡∏ó‡πå
with open('/content/drive/MyDrive/thai_loanwords_new_update(1).json', 'r', encoding='utf-8') as f:
     loanwords_data = json.load(f)
     loanwords_whitelist = set(item['thai_word'] for item in loanwords_data)

# ‡πÇ‡∏´‡∏•‡∏î common misspellings JSON
with open('/content/drive/MyDrive/update_common_misspellings.json', 'r', encoding='utf-8') as f:
    data = json.load(f)
    COMMON_MISSPELLINGS = {item['wrong']: item['right'] for item in data}

# ‡πÇ‡∏´‡∏•‡∏î list ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏â‡∏µ‡∏Å‡∏Ñ‡∏≥‡πÑ‡∏î‡πâ
with open("/content/drive/MyDrive/splitable_phrases.json", "r", encoding="utf-8") as f:
    splitable_phrases = set(json.load(f))  # ‚úÖ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô set ‡πÄ‡∏™‡∏°‡∏≠

API_KEY = '33586c7cf5bfa0029887a9981bf94963' # add Apikey
API_URL = 'https://api.longdo.com/spell-checker/proof'

custom_words = {"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢", "‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤", "‡∏ô‡∏≤‡∏ô‡∏≤‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏£"}

#‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏â‡∏µ‡∏Å‡∏Ñ‡∏≥‡πÑ‡∏î‡πâ
strict_not_split_words = { '‡∏°‡∏≤‡∏Å‡∏°‡∏≤‡∏¢', '‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢', '‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠' }

thai_dict = set(w for w in set(thai_words()).union(custom_words) if (' ' not in w) and w.strip())

# allowed punctuation (‡πÄ‡∏û‡∏¥‡πà‡∏° ' ‡πÅ‡∏•‡∏∞ ")
allowed_punctuations = [ '.', ',', '-', '(', ')', '!', '?', '%', '‚Äú', '‚Äù', '‚Äò', '‚Äô', '"', "'", '‚Ä¶', '‡∏Ø' ]

# Allow / Forbid list ‡πÑ‡∏°‡πâ‡∏¢‡∏°‡∏Å (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢)
allow_list = {'‡∏õ‡∏µ', '‡∏≠‡∏∑‡πà‡∏ô', '‡πÄ‡∏•‡πá‡∏Å', '‡πÉ‡∏´‡∏ç‡πà', '‡∏°‡∏≤‡∏Å', '‡∏´‡∏•‡∏≤‡∏¢', '‡∏ä‡πâ‡∏≤', '‡πÄ‡∏£‡πá‡∏ß', '‡∏ä‡∏±‡∏î', '‡∏î‡∏µ', '‡∏ú‡∏¥‡∏î' ,'‡πÄ‡∏™‡∏µ‡∏¢', '‡∏´‡∏≤‡∏¢','‡∏™‡∏ß‡∏¢','‡∏°‡∏±‡πà‡∏ß','‡∏á‡πà‡∏≤‡∏¢'}
forbid_list = {'‡∏ô‡∏≤', '‡∏ö‡∏≤‡∏á‡∏Ñ‡∏ô', '‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á', '‡∏ö‡∏≤‡∏á‡∏™‡∏¥‡πà‡∏á', '‡∏ö‡∏≤‡∏á‡∏Å‡∏£‡∏ì‡∏µ'}

# ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ
splitable_pairs = { ("‡πÑ‡∏õ", "‡∏°‡∏≤"),("‡πÑ‡∏î‡πâ","‡∏™‡πà‡∏ß‡∏ô"),("‡∏î‡∏µ","‡πÅ‡∏ï‡πà") }

#------------------------------------------------------------------------#
# #‡∏ï‡∏£‡∏ß‡∏à‡∏Å‡∏≤‡∏£‡∏â‡∏µ‡∏Å‡∏Ñ‡∏≥
def check_linebreak_issue(prev_line_tokens, next_line_tokens, max_words=3):
    last_word = prev_line_tokens[-1]
    first_word = next_line_tokens[0]
    if last_word.endswith('-') or first_word.startswith('-'):
        return False, None, None, None
    for prev_n in range(1, min(max_words, len(prev_line_tokens)) + 1):
        prev_part = ''.join(prev_line_tokens[-prev_n:])
        for next_n in range(1, min(max_words, len(next_line_tokens)) + 1):
            next_part = ''.join(next_line_tokens[:next_n])
            combined = normalize(prev_part + next_part)
            if ( (' ' not in combined) and (combined not in splitable_phrases) and ( (combined in strict_not_split_words) or ( (combined in thai_dict) and (len(word_tokenize(combined, engine='newmm')) == 1) ) ) ):
                return True, prev_part, next_part, combined
    return False, None, None, None

#‡∏ß‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡∏•‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
def analyze_linebreak_issues(text):
    lines = text.strip().splitlines()
    issues = []
    for i in range(len(lines) - 1):
        prev_line = lines[i].strip()
        next_line = lines[i + 1].strip()
        prev_tokens = word_tokenize(prev_line)
        next_tokens = word_tokenize(next_line)
        if not prev_tokens or not next_tokens:
            continue
        issue, prev_part, next_part, combined = check_linebreak_issue(prev_tokens, next_tokens)
        if issue:
            issues.append({
                           'line_before': prev_line,
                           'line_after': next_line,
                           'prev_part': prev_part,
                           'next_part': next_part,
                           'combined': combined,
                           'pos_in_text': (i, len(prev_tokens)) })
    return issues

#‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
def merge_linebreak_words(text, linebreak_issues):
    lines = text.splitlines()
    for issue in reversed(linebreak_issues):
        i, _ = issue['pos_in_text']
        lines[i] = lines[i].rstrip() + issue['combined'] + lines[i+1].lstrip()[len(issue['next_part']):]
        lines.pop(i+1)
    return "\n".join(lines) # Added return statement here

#‡∏Ç‡πâ‡∏≤‡∏°‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
def is_english_or_number(word: str) -> bool:
    """ ‡∏Ñ‡∏∑‡∏ô True ‡∏ñ‡πâ‡∏≤ word ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç """
    w = word.strip()
    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ A-Z, a-z, ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 0-9 ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡πÅ‡∏ï‡πà‡∏û‡∏ß‡∏Å .,()-_/ ‡∏õ‡∏ô
    return bool(re.fullmatch(r"[A-Za-z0-9().,\-_/]+", w))

# 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP + Longdo
# # -------------------------------
def pythainlp_spellcheck(tokens, pos_tags, dict_words, ignore_words):
    """ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP ‡∏Ñ‡∏∑‡∏ô list ‡∏Ç‡∏≠‡∏á dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ 'word', 'pos', 'index' """
    mistakes = []
    for i, token in enumerate(tokens):
        # Skip empty tokens, numbers, and English words
        if not token or is_english_or_number(token):
            continue

        # Check if the word is in our custom dictionary
        if token in ignore_words:
            continue

        # Check if the word is in the general Thai dictionary
        if token in dict_words:
            continue
        # Use PyThaiNLP spell checker
        suggestions = spell(token)

        # If no suggestions and not in dictionaries, consider it a potential error
        if not suggestions:
            mistakes.append({
                             'word': token,
                             'pos': pos_tags[i][1] if i < len(pos_tags) else None,
                             'index': i, 'suggestions': suggestions })
    return mistakes

def spellcheck_before_tokenize(text):
    words = re.findall(r'[‡∏Å-‡πô]+', text) # ‚úÖ ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡πÑ‡∏ó‡∏¢
    pos_tags = pos_tag(words, corpus='orchid')

    # filter ‡∏Ç‡πâ‡∏≤‡∏°‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©/‡πÄ‡∏•‡∏Ç ‡∏≠‡∏µ‡∏Å‡∏ä‡∏±‡πâ‡∏ô
    words = [w for w in words if not is_english_or_number(w)]

    pythai_errors = pythainlp_spellcheck(
                                         words, pos_tags,
                                         dict_words=thai_dict,
                                         ignore_words=custom_words )
    wrong_words = [e['word'] for e in pythai_errors]

    longdo_results = longdo_spellcheck_batch(wrong_words)
    spelling_errors_legit = [
                             {**e, 'suggestions': longdo_results.get(e['word'], [])}
                             for e in pythai_errors if e['word'] in longdo_results ]
    return spelling_errors_legit

#longdo spell checker
def longdo_spellcheck_batch(words):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏î‡πâ‡∏ß‡∏¢ Longdo API ‡πÅ‡∏ö‡∏ö batch
    ‡∏Ñ‡∏∑‡∏ô dict {word: [suggestions]}
    """
    results = {}
    headers = {"Content-Type": "application/json"}
    for word in words:
        try:
            payload = {
                "text": word,
                "api": API_KEY
            }
            response = requests.post(API_URL, headers=headers, data=json.dumps(payload))
            if response.status_code == 200:
                data = response.json()
                suggestions = []
                # Longdo ‡∏à‡∏∞‡∏™‡πà‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥
                if "words" in data and data["words"]:
                    for w in data["words"]:
                        if "candidates" in w:
                            suggestions.extend(c["text"] for c in w["candidates"])
                results[word] = suggestions
            else:
                results[word] = []
        except Exception as e:
            results[word] = []
    return results

# ‡∏ï‡∏£‡∏ß‡∏à common misspellings ‡∏Å‡πà‡∏≠‡∏ô tokenize (‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏¥‡∏ö)
def check_common_misspellings_before_tokenize(text, misspelling_dict):
    """ text : ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏¥‡∏ö (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà tokenize) misspelling_dict : dict ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON ‡∏Ñ‡∏∑‡∏ô list ‡∏Ç‡∏≠‡∏á dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ 'word', 'index', 'right' """
    errors = []
    for wrong, right in misspelling_dict.items():
        if wrong in text:
            for m in re.finditer(re.escape(wrong), text):
                errors.append({
                               "word": wrong,
                               "index": m.start(), # ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏¥‡∏ö
                               "right": right })
    return errors

# ‡∏ï‡∏£‡∏ß‡∏à loanwords ‡∏Å‡πà‡∏≠‡∏ô tokenize
def check_loanword_before_tokenize(words, whitelist):
    mistakes = []
    for i, w in enumerate(words):
        if is_english_or_number(w):  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©+‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
            continue
        matches = difflib.get_close_matches(w, list(whitelist), n=1, cutoff=0.7)
        if matches and w not in whitelist:
            mistakes.append({
                'word': w,                  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å 'found'
                'index': i,
                'suggestions': [matches[0]] # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å 'should_be'
            })
    return mistakes

#‡∏ï‡∏£‡∏ß‡∏à‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï
allowed_phrases = ["‡∏Ø‡∏•‡∏Ø"]

# ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö
def find_unallowed_punctuations(text):
    # ‡∏•‡∏ö‡∏ß‡∏•‡∏µ‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏£‡∏ß‡∏à
    for phrase in allowed_phrases:
        text = text.replace(phrase, "")

    pattern = f"[^{''.join(re.escape(p) for p in allowed_punctuations)}a-zA-Z0-9‡∏Å-‡πô\\s]"
    return list(set(re.findall(pattern, text)))

#‡πÉ‡∏ä‡πâ‡πÅ‡∏¢‡∏Å‡πÑ‡∏°‡πâ‡∏¢‡∏°‡∏Å‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô
def separate_maiyamok(text):
    return re.sub(r'(\S+?)‡πÜ', r'\1 ‡πÜ', text)

#‡∏ï‡∏£‡∏ß‡∏à‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πâ‡∏¢‡∏°‡∏Å
def analyze_maiyamok(tokens, pos_tags):
    results = []
    found_invalid = False
    VALID_POS = {'NCMN', 'NNP', 'VACT', 'VNIR', 'CLFV', 'ADVN', 'ADVI', 'ADVP', 'PRP', 'ADV'}
    for i, token in enumerate(tokens):
        if token == '‡πÜ':
            prev_idx = i - 1
            prev_word = tokens[prev_idx] if prev_idx >= 0 else None
            prev_tag = pos_tags[prev_idx][1] if prev_idx >= 0 else None
            if prev_word is None or prev_word == '‡πÜ': verdict = "‚ùå ‡πÑ‡∏°‡πâ‡∏¢‡∏°‡∏Å‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ/‡∏Ñ‡∏≥"
            elif prev_word in forbid_list: verdict = '‚ùå ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πâ‡∏¢‡∏°‡∏Å‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏µ‡πâ'
            elif (prev_tag in VALID_POS) or (prev_word in allow_list): verdict = '‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πâ‡∏¢‡∏°‡∏Å‡∏ã‡πâ‡∏≥‡∏Ñ‡∏≥‡πÑ‡∏î‡πâ)'
            else: verdict = '‚ùå ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πâ‡∏¢‡∏°ok ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏≤‡∏°/‡∏Å‡∏£‡∏¥‡∏¢‡∏≤/‡∏ß‡∏¥‡πÄ‡∏®‡∏©‡∏ì‡πå'
            context = tokens[max(0, i-2):min(len(tokens), i+3)]
            results.append({
                            '‡∏Ñ‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡πÑ‡∏°‡πâ‡∏¢‡∏°‡∏Å': prev_word or '',
                            'POS ‡∏Ñ‡∏≥‡∏Å‡πà‡∏≠‡∏ô': prev_tag or '',
                            '‡∏ö‡∏£‡∏¥‡∏ö‡∏ó': ' '.join(context),
                            '‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞': verdict })
            if verdict.startswith('‚ùå'): found_invalid = True
    return results, found_invalid

#‡∏ï‡∏£‡∏ß‡∏à‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥
def detect_split_errors(tokens, custom_words=None, splitable_phrases=None):
    check_dict = set(thai_words()).union(custom_words or [])
    check_dict = {w for w in check_dict if (' ' not in w) and w.strip()}
    splitable_phrases = splitable_phrases or set()

    errors = []
    for i in range(len(tokens) - 1):
        combined = tokens[i] + tokens[i + 1]

        # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤ combined ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô dict
        # ‚ùå ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡πÅ‡∏¢‡∏Å ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô splitable_pairs
        if (combined in check_dict) and ((tokens[i], tokens[i+1]) not in splitable_pairs):
            errors.append({
                           "split_pair": (tokens[i], tokens[i+1]),
                           "suggested": combined })
    return errors

#‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç
def evaluate_text(text):
    # -----------------------------
    # ‚úÖ ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
    # -----------------------------
    if not text or not text.strip():
        return {
            'linebreak_issues': [],
            'spelling_errors': [],
            'loanword_spell_errors': [],
            'punctuation_errors': [],
            'maiyamok_results': [],
            'split_errors': [],
            'reasons': ["‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö"],
            'score': 0.0,
            'total_error_count': 0
        }

    # ‚úÖ ‡∏ñ‡πâ‡∏≤‡∏ô‡∏±‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß ‚â§ 2 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡πâ 0
    lines = [l for l in text.strip().splitlines() if l.strip()] # Define lines here
    char_count = len(text.replace(" ", "").replace("\n", "")) # Calculate char count here
    if len(lines) <= 2 and char_count < 50:
        return {
            'linebreak_issues': [],
            'spelling_errors': [],
            'loanword_spell_errors': [],
            'punctuation_errors': [],
            'maiyamok_results': [],
            'split_errors': [],
            'reasons': [f"‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ({len(lines)} ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î, {char_count} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)"],
            'score': 0.0,
            'total_error_count': 0
        }
    # -----------------------------
    # 1) ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
    # -----------------------------
    linebreak_issues = analyze_linebreak_issues(text)
    corrected_text = merge_linebreak_words(text, linebreak_issues)

    # ‚úÖ ‡∏•‡∏ö newline ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (‡∏Å‡∏£‡∏ì‡∏µ merge_linebreak_words ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏à‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÄ‡∏Ñ‡∏™)
    corrected_text = corrected_text.replace("\n", "")

    # ‚úÖ normalize ‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏£‡∏ß‡∏à
    corrected_text = normalize(corrected_text)

    # 2) ‡∏ï‡∏£‡∏ß‡∏à common misspellings ‡∏Å‡πà‡∏≠‡∏ô tokenize
    json_misspells = check_common_misspellings_before_tokenize(corrected_text, COMMON_MISSPELLINGS)

    # 3) tokenize ‡∏õ‡∏Å‡∏ï‡∏¥
    tokens = [t for t in word_tokenize(corrected_text, engine='newmm', keep_whitespace=False)
              if not is_english_or_number(t)]
    pos_tags = pos_tag(tokens, corpus='orchid')

    # ‡∏ï‡∏£‡∏ß‡∏à spelling ‡∏î‡πâ‡∏ß‡∏¢ PyThaiNLP
    pythai_errors = pythainlp_spellcheck(tokens, pos_tags, dict_words=thai_dict, ignore_words=custom_words)

    # ‡∏ï‡∏£‡∏ß‡∏à loanwords (‡∏™‡∏≠‡∏á‡∏ä‡∏±‡πâ‡∏ô)
    # -----------------------------

    # 1) ‡∏ï‡∏£‡∏ß‡∏à‡πÅ‡∏ö‡∏ö‡∏î‡∏¥‡∏ö (‡∏à‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ)
    raw_words = re.findall(r'[‡∏Å-‡πô]+', corrected_text)
    raw_loanword_errors = check_loanword_before_tokenize(raw_words, loanwords_whitelist)

    # 2) ‡∏ï‡∏£‡∏ß‡∏à‡πÅ‡∏ö‡∏ö tokenize (‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö tokenizer)
    token_loanword_errors = check_loanword_before_tokenize(tokens, loanwords_whitelist)

    # ‡∏£‡∏ß‡∏°‡∏ú‡∏•
    loanword_errors = raw_loanword_errors + token_loanword_errors

    # ‡∏ï‡∏£‡∏ß‡∏à Longdo
    longdo_results = longdo_spellcheck_batch([e['word'] for e in pythai_errors])
    longdo_errors = [
        {
            **e,
            'suggestions': [str(s) for s in longdo_results.get(e['word'], []) if s]  # ‚úÖ sanitize
        }
        for e in pythai_errors
    ]

    # ‡∏£‡∏ß‡∏° spelling errors + sanitize suggestions ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô string ‡πÄ‡∏™‡∏°‡∏≠
    all_spelling_errors = longdo_errors + [
        {
            "word": e["word"],
            "pos": None,
            "index": e["index"],
            "suggestions": [str(e["right"])] if e["right"] else [],
        }
        for e in json_misspells
    ] + [
        {
            "word": e["word"],
            "pos": None,
            "index": e["index"],
            "suggestions": [str(s) for s in e.get("suggestions", []) if s],
        }
        for e in loanword_errors
    ]

    # ‡∏ï‡∏£‡∏ß‡∏à punctuation, maiyamok, split word
    punct_errors = find_unallowed_punctuations(corrected_text)
    maiyamok_results, has_wrong_maiyamok = analyze_maiyamok(tokens, pos_tags)
    split_errors = detect_split_errors(tokens, custom_words=custom_words)

    # -----------------------------
    # ‚úÖ ‡πÉ‡∏´‡∏°‡πà: ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏ã‡πâ‡∏≥‡πÄ‡∏õ‡πá‡∏ô 1
    unique_spelling_words = {e["word"] for e in all_spelling_errors}
    unique_split_errors = {e["suggested"] for e in split_errors}

    total_errors = (
        len(unique_spelling_words) +
        len(linebreak_issues) +
        len(unique_split_errors) +
        len(punct_errors) +
        sum(1 for r in maiyamok_results if r['‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞'].startswith('‚ùå'))
    )

    # -----------------------------
    # ‡∏£‡∏ß‡∏° reasons
    # -----------------------------
    reasons = []
    if linebreak_issues:
        details = [f"{issue['prev_part']} + {issue['next_part']} ‚Üí {issue['combined']}" for issue in linebreak_issues]
        reasons.append("‡∏û‡∏ö‡∏Å‡∏≤‡∏£‡∏â‡∏µ‡∏Å‡∏Ñ‡∏≥‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î: " + "; ".join(details))
    if split_errors:
        details = [f"{e['split_pair'][0]} + {e['split_pair'][1]} ‚Üí {e['suggested']}" for e in split_errors]
        reasons.append("‡∏û‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î: " + "; ".join(details))
    if all_spelling_errors:
        error_words = [
            f"{e['word']} (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: {', '.join(str(s) for s in e.get('suggestions', []) if s)})"
            for e in all_spelling_errors
        ]
        reasons.append("‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏à‡∏≠‡∏Ñ‡∏≥‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏±‡∏ö‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ú‡∏¥‡∏î: " + ", ".join(error_words))
    if punct_errors:
        reasons.append(f"‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï: {', '.join(punct_errors)}")
    wrong_desc = [x for x in maiyamok_results if x['‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞'].startswith('‚ùå')]
    if wrong_desc:
        texts = [f"{x['‡∏Ñ‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡πÑ‡∏°‡πâ‡∏¢‡∏°‡∏Å']}: {x['‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞']}" for x in wrong_desc]
        reasons.append("‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πâ‡∏¢‡∏°‡∏Å‡∏ú‡∏¥‡∏î: " + '; '.join(texts))
    if not reasons:
        reasons.append("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤")

    # -----------------------------
    # ‚úÖ ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÅ‡∏ö‡∏ö 5 ‡∏£‡∏∞‡∏î‡∏±‡∏ö
    # -----------------------------
    if total_errors == 0:
        score = 2.0
    elif total_errors == 1:
        score = 1.5
    elif total_errors == 2:
        score = 1.0
    elif total_errors == 3:
        score = 0.5
    else:  # 4 ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ
        score = 0.0

    # Ensure a dictionary is always returned
    return {
        'linebreak_issues': linebreak_issues,
        'spelling_errors': all_spelling_errors,
        'loanword_spell_errors': loanword_errors,
        'punctuation_errors': list(punct_errors),
        'maiyamok_results': maiyamok_results,
        'split_errors': split_errors,
        'reasons': reasons,
        'score': score,
        'total_error_count': total_errors,
        'char_count': char_count # Add char count to the return dictionary
    }

def evaluate_single_answer(answer_text):
    res = evaluate_text(str(answer_text))
    spelling_score = res["score"]   # ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡πÄ‡∏Å‡∏• 0,0.5,1,1.5,2
    spelling_reason = res["reasons"]

    result = {
        "‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏Ñ‡∏≥ (2 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)": spelling_score,
        "‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î": spelling_reason,
        "‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î": spelling_score
    }
    return json.dumps(result, ensure_ascii=False, indent=2)

# -------CSV---------
df = pd.read_csv("/content/drive/MyDrive/dataset_S11_‡πÉ‡∏´‡∏°‡πà(Sheet1).csv")

‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏∞‡∏Å‡∏î_list = []
‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ú‡∏¥‡∏î_list = []

for i, row in df.iterrows():
    text = row["student_answer_2"]
    if not isinstance(text, str):
        text = ""

    result = evaluate_text(text)
    ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏∞‡∏Å‡∏î_list.append(result["score"])
    ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ú‡∏¥‡∏î_list.append("; ".join(result.get("reasons", [])))  # ‚úÖ ‡πÅ‡∏Å‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ

df["S11"] = ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏∞‡∏Å‡∏î_list
df["‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î"] = ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ú‡∏¥‡∏î_list

df.to_csv("/content/sample_data/score14_s11.csv", index=False, encoding="utf-8-sig")
print("‚úÖ ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏Ñ‡∏≥ (S11)")
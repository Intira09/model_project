# -*- coding: utf-8 -*-
"""S4_สะกดคำ(ใหม่ล่าสุด).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yGto4H5N0_g8dqXzv8X9CmjBTta9f6BU

# คำทับศัพท์
"""

import pandas as pd
import json

# ----------------------
# รายการคำทับศัพท์ตัวอย่าง
# ----------------------
# เพิ่มคำได้ตามต้องการในรูปแบบ (thai_word, transliteration, category)

loanwords_list = [
    ("คอมพิวเตอร์", "computer"),
    ("อินเทอร์เน็ต", "internet"),
    ("ซอฟต์แวร์", "software"),
    ("ฮาร์ดแวร์", "hardware"),
    ("สมาร์ทโฟน", "smartphone"),
    ("แอปพลิเคชัน", "application"),
    ("อีเมล", "email"),
    ("เซิร์ฟเวอร์", "server"),
    ("บราวเซอร์", "browser"),
    ("ดาต้าเบส", "database"),
    ("เฟซบุ๊ก", "facebook"),
    ("วิดีโอ","video"),
    ("อินสตาแกรม","instagram"),
    ("ไฟล์","file"),
    ("แชต","chat"),
    ("แช็ต","chat"),
    ("ยูทูบ","youtube"),
    ("โซเชียล","social"),
    ("กูเกิล","google"),
    ("ทวิตเตอร์","twitter"),
    ("ช้อปปี้","shopee"),
    ("ติ๊กต๊อก","tiktok"),
    ("ไลน์","line"),
    ("โซเชียลมีเดีย","social media"),
    ("ออนไลน์","online"), ("คอนเทนต์","content"), ("คอมเมนต์","comment"), ("ไลก์","like"), ("แชร์","share"),
    ("แฮชแท็ก","hastag"), ("โปรโมชัน","promotion"), ("โพรโมต","promote"), ("โพรไฟล์","profile"), ("แพลตฟอร์ม","platform"),
    ("แพล็ตฟอร์ม","platform"), ("โพสต์","post"), ("อินฟลูเอนเซอร์","influencer"), ("วิดีโอคอล","video call"),
    ("ลิงก์","link"), ("แอ็กเคานต์","account"), ("ฟีเจอร์","feature"), ("อัปเดต","update"), ("เวอร์ชัน","version"),
    ("โน้ตบุ๊ก","notebook"), ("แท็บเล็ต","tablet"), ("ดิจิทัล","digital"),("โปรแกรม","program"),("ไลฟ์","live"),("เกม","game"),
    ("แอป","app"),("แอ็ป","app"),("การ์ตูน","cartoon"),("คอลเซนเตอร์","call center"),
]

# ----------------------
# แปลงเป็น DataFrame
# ----------------------
df = pd.DataFrame(loanwords_list, columns=["thai_word", "transliteration"])

# ----------------------
# บันทึกเป็น CSV
# ----------------------
csv_path = "thai_loanwords_new_update.csv"
df.to_csv(csv_path, index=False, encoding="utf-8-sig")

# ----------------------
# บันทึกเป็น JSON
# ----------------------
json_path = "thai_loanwords_new_update(1).json"
with open(json_path, "w", encoding="utf-8") as f:
    json.dump(df.to_dict(orient="records"), f, ensure_ascii=False, indent=2)

print(f"บันทึกไฟล์แล้ว:\n- {csv_path}\n- {json_path}")

"""# สร้างคำที่มักสะกดผิดแต่ pythainlp ตรวจไม่เจอ"""

import pandas as pd
import json

# ----------------------
# รายการคำทับศัพท์ตัวอย่าง
# ----------------------
# เพิ่มคำได้ตามต้องการในรูปแบบ (thai_word, transliteration, category)

common_misspellings = [
  {"wrong": "สังเกตุ", "right": "สังเกต"},
  {"wrong": "ภูมูิคุ้มกัน", "right": "ภูมิคุ้มกัน"},
  {"wrong": "กระจ่าย", "right": "กระจาย"},
  {"wrong": "แก่ปัญหา", "right": "แก้ปัญหา"},
  {"wrong": "เจดนา", "right": "เจตนา"},
  {"wrong": "หริอ", "right": "หรือ"},
  {"wrong": "ส่อออนไลน์", "right": "สื่อออนไลน์"},
  {"wrong": "วิพากวิจารณ์", "right": "วิพากษ์วิจารณ์"},
  {"wrong": "โปรแกลม", "right": "โปรแกรม"},
  {"wrong": "คอมพิวเตอร์์", "right": "คอมพิวเตอร์"},
  {"wrong": "สัณญาณ", "right": "สัญญาณ"},
  {"wrong": "สัญญาญ", "right": "สัญญาณ"},
  {"wrong": "สัณญาญ", "right": "สัญญาณ"},
  {"wrong": "เหตุผล์", "right": "เหตุผล"},
  {"wrong": "ลำบาก์", "right": "ลำบาก"},
  {"wrong": "ทัศนะคติ", "right": "ทัศนคติ"},
  {"wrong": "รัปทาน", "right": "รับประทาน"},
  {"wrong": "ศาสนาพุทธท์", "right": "ศาสนาพุทธ"},
  {"wrong": "สัพเพ", "right": "สัพเพ"},
  {"wrong": "ควบคู่์", "right": "ควบคู่"},
  {"wrong": "อัธยาสัยย์", "right": "อัธยาศัย"},
  {"wrong": "เผอเร๊อ", "right": "เผลอเรอ"},
  {"wrong": "คณิตศาตร์", "right": "คณิตศาสตร์"},
  {"wrong": "จริญ", "right": "เจริญ"},
  {"wrong": "เพาะว่า", "right": "เพราะว่า"},
  {"wrong": "ยกเว้น์", "right": "ยกเว้น"},
  {"wrong": "พุทธศาสน์", "right": "พุทธศาสนา"},
  {"wrong": "สังคมณ์", "right": "สังคม"},
  {"wrong": "ใส่ใจ์", "right": "ใส่ใจ"},
  {"wrong": "สัณฐาน", "right": "สัณฐาน"},
  {"wrong": "กฏหมาย", "right": "กฎหมาย"},
  {"wrong": "ธุระกิจ", "right": "ธุรกิจ"},
  {"wrong": "ตัวอยาง", "right": "ตัวอย่าง"},
  {"wrong": "ล้อแหลม", "right": "ล่อแหลม"},
  {"wrong": "หลงไหล", "right": "หลงใหล"},
  {"wrong": "ชื้อ", "right": "ซื้อ"} ,
  {"wrong": "โทบ", "right": "โทษ"},
  {"wrong": "รับผืดชอบ", "right": "รับผิดชอบ"},
  {"wrong": "แพร่กระจ่าย", "right": "แพร่กระจาย"},
  {"wrong": "ข้อมุลข่าวสาร", "right": "ข้อมูลข่าวสาร"},
  {"wrong": "ขาวสาร", "right": "ข่าวสาร"},
  {"wrong": "สํานักพิม", "right": "สำนักพิมพ์"},
  {"wrong": "คุนภาพ", "right": "คุณภาพ"},
  {"wrong": "ระมัตระวัง", "right": "ระมัดระวัง"},
  {"wrong": "การพันนัน", "right": "การพนัน"},
  {"wrong": "ก้าวกระโดต", "right": "ก้าวกระโดด"},
  {"wrong": "อาชิพ", "right": "อาชีพ"},
  {"wrong": "โฆษณาชวนเชื่อะ", "right": "โฆษณาชวนเชื่อ"},
  {"wrong": "แอบแฟง", "right": "แอบแฝง"},
  {"wrong": "ผลประโยด", "right": "ผลประโยชน์"},
  {"wrong": "เสื่อมเสย", "right": "เสื่อมเสีย"},
  {"wrong": "น่าเชื่อถื่อ", "right": "น่าเชื่อถือ"},
  {"wrong": "วิทยาศาตร์", "right": "วิทยาศาสตร์"},
  {"wrong": "วิทยาศาสตร์์", "right": "วิทยาศาสตร์"},
  {"wrong": "เสรษฐกิจ", "right": "เศรษฐกิจ"},
  {"wrong": "เศษฐกิจก", "right": "เศรษฐกิจ"},
  {"wrong": "ปรัชยา", "right": "ปรัชญา"},
  {"wrong": "มหาลัยวิทยาลัย", "right": "มหาวิทยาลัย"},
  {"wrong": "มหาวิธาลัย", "right": "มหาวิทยาลัย"},
  {"wrong": "อุสาหกรรม", "right": "อุตสาหกรรม"},
  {"wrong": "อุสสาหกรรม", "right": "อุตสาหกรรม"},
  {"wrong": "พัฒณา", "right": "พัฒนา"},
  {"wrong": "นะโยบาย", "right": "นโยบาย"},
  {"wrong": "เกิดขื้น", "right": "เกิดขึ้น"},
  {"wrong": "ประโยคน์", "right": "ประโยค"},
  {"wrong": "กำลังไจ", "right": "กำลังใจ"},
  {"wrong": "ทเลาะ", "right": "ทะเลาะ"},
  {"wrong": "ทะเราะ", "right": "ทะเลาะ"},
  {"wrong": "อันตราย์", "right": "อันตราย"},
  {"wrong": "อารม", "right": "อารมณ์"},
  {"wrong": "ปัญยา", "right": "ปัญญา"},
  {"wrong": "ประโยขน์", "right": "ประโยชน์"},
  {"wrong": "เมิอง", "right": "เมือง"},
  {"wrong": "ข้อจากัก", "right": "ข้อจำกัด"},
  {"wrong": "เริอง", "right": "เรื่อง"},
  {"wrong": "แระ", "right": "และ"},
  {"wrong": "ด้าย", "right": "ด้วย"},
  {"wrong": "สิอ", "right": "สื่อ"},{"wrong": "สิง", "right": "สิ่ง"},
  {"wrong": "เส่อมเสีย", "right": "เสื่อมเสีย"},{"wrong": "หืรือ", "right": "หรือ"},
  {"wrong": "งานเขียง", "right": "งานเขียน"},{"wrong": "จิตนา", "right": "เจตนา"},
  {"wrong": "หน้าเชื่อถือ", "right": "น่าเชื่อถือ"},{"wrong": "หน้ากลัว", "right": "น่ากลัว"},
  {"wrong": "มิตรฉาชีพ", "right": "มิจฉาชีพ"},{"wrong": "ทึง", "right": "ถึง"},
  {"wrong": "ประโยขน์", "right": "ประโยชน์"},{"wrong": "แสอง", "right": "แสวง"},
  {"wrong": "ข่วว", "right": "ข่าว"},{"wrong": "มันใจ", "right": "มั่นใจ"},{"wrong": "ปรโยชน์", "right": "ประโยชน์"},
  {"wrong": "ข่าฉสาร", "right": "ข่าวสาร"},{"wrong": "แฟง", "right": "แฝง"},{"wrong": "สดวก", "right": "สะดวก"},
  {"wrong": "ด้ย", "right": "ด้วย"},{"wrong": "โคเซ็นเตอร์", "right": "คอลเซนเตอร์"},{"wrong": "เหนื่ย", "right": "เหยื่อ"},
  {"wrong": "มีจาชีพ", "right": "มิจฉาชีพ"},{"wrong": "แอพพิเคชั้น", "right": "แอปพลิเคชัน"},
  {"wrong": "โทรทัส", "right": "โทรทัศน์"},{"wrong": "สะบาย", "right": "สบาย"},
  {"wrong": "จิ๋กโก๋", "right": "จิ๊กโก๋"},{"wrong": "คอบเขต", "right": "ขอบเขต"},{"wrong": "มิฉฉาชีพ", "right": "มิจฉาชีพ"},
  {"wrong": "น่าเชื้อ", "right": "น่าเชื่อ"},{"wrong": "มิชาชีพ", "right": "มิจฉาชีพ"},
  {"wrong": "มิวชาชีพ", "right": "มิจฉาชีพ"},{"wrong": "ขำเป็น", "right": "จำเป็น"},
  {"wrong": "สิ่นค้า", "right": "สินค้า"},{"wrong": "เทีจ", "right": "เท็จ"},{"wrong": "แพ่", "right": "แพร่"},
  {"wrong": "เครื่องจิก", "right": "เครื่องจักร"},{"wrong": "โฆณา", "right": "โฆษณา"},
  {"wrong": "เหยือ", "right": "เหยื่อ"},{"wrong": "กระแส่", "right": "กระแส"},{"wrong": "คนหาข้อมูล", "right": "ค้นหาข้อมูล"},
  {"wrong": "สร่าง", "right": "สร้าง"},{"wrong": "ออนไลย์", "right": "ออนไลน์"},
  {"wrong": "โซษณา", "right": "โฆษณา"},{"wrong": "เหยื้อ", "right": "เหยื่อ"},{"wrong": "สะบาย", "เรีว": "เร็ว"},
  {"wrong": "เพอ่", "right": "เพื่อ"},{"wrong": "จดั", "right": "จัด"},{"wrong": "เหยอื่", "right": "เหยื่อ"},
  {"wrong": "โคลอเซ็นเตอร์", "right": "คอลเซนเตอร์"},{"wrong": "ฉาจ", "right": "อาจ"},
  {"wrong": "หรือป่าว", "right": "หรือเปล่า"},{"wrong": "เท่าถับ", "right": "เท่ากับ"},
  {"wrong": "อย่าก", "right": "อยาก"},{"wrong": "โพช", "right": "โพสต์"},
  {"wrong": "เกส์", "right": "เกม"},{"wrong": "ป้องกับ", "right": "ป้องกัน"},{"wrong": "มิฉาชีพ", "right": "มิจฉาชีพ"},
  {"wrong": "เดือฉ", "right": "เดือด"},{"wrong": "แวป", "right": "เว็บ"},{"wrong": "ขอ้", "right": "ข้อ"},
  {"wrong": "วิพากพ์วิจาร", "right": "วิพากษ์วิจารณ์"},{"wrong": "หลอกล่วง", "right": "หลอกลวง"},
  {"wrong": "เปลื่อย", "right": "เปลือย"},{"wrong": "เยอั", "right": "เยอะ"},{"wrong": "ล้วงหน้า", "right": "ล่วงหน้า"},{"wrong": "หรัส", "right": "รหัส"},
  {"wrong": "ยุคสมิัย", "right": "ยุคสมัย"},{"wrong": "ผู้ถึง", "right": "พูดถึง"},{"wrong": "มิประโยชน์", "right": "มีประโยชน์"},
  {"wrong": "เปิน", "right": "เป็น"},{"wrong": "เพราฉะนั้น", "right": "เพราะฉะนั้น"},{"wrong": "ค้าขาบ", "right": "ค้าขาย"},
  {"wrong": "เชื่ลฟัง", "right": "เชื่อฟัง"},{"wrong": "รูบแบบ", "right": "รูปแบบ"},{"wrong": "เพส", "right": "เพศ"},{"wrong": "แชท", "right": "แชต"},
  {"wrong": "ก้เล่น", "right": "ก็เล่น"},{"wrong": "ก้จะ", "right": "ก็จะ"},{"wrong": "ข้วมูล", "right": "ข้อมูล"},{"wrong": "เป้น", "right": "เป็น"},
  {"wrong": "แสวงทา", "right": "แสวงหา"},{"wrong": "กังวน", "right": "กังวล"},{"wrong": "เรือง", "right": "เรื่อง"},
  {"wrong": "เทคโนโลยีด", "right": "เทคโนโลยี"},{"wrong": "คอลเซ็นเตอร์", "right": "คอลเซนเตอร์"},{"wrong": "สัวคม", "right": "สังคม"},
  {"wrong": "สิ่อ", "right": "สื่อ"},{"wrong": "สื่ล", "right": "สื่อ"},{"wrong": "รวดเริว", "right": "รวดเร็ว"},{"wrong": "เสื่อมเลีย", "right": "เสื่อมเสีย"},
  {"wrong": "สัคม", "right": "สังคม"},{"wrong": "กลุมคน", "right": "กลุ่มคน"},{"wrong": "ที่มิ", "right": "ที่มี"},
  {"wrong": "พลดี", "right": "ผลดี"},{"wrong": "เหยี่อ", "right": "เหยื่อ"},{"wrong": "ใช่อย่างระวัง", "right": "ใช้อย่างระวัง"},
  {"wrong": "ชีวิติ", "right": "ชีวิต"},{"wrong": "เข่าถึง", "right": "เข้าถึง"},{"wrong": "เหยิ่อ", "right": "เหยื่อ"},
  {"wrong": "ตลาค", "right": "ตลาด"},{"wrong": "สื่ยหาย", "right": "เสียหา"},{"wrong": "ศิลธรรม", "right": "ศีลธรรม"},
  {"wrong": "พึง", "right": "พึ่ง"},{"wrong": "นอกงาก", "right": "นอกจาก"},{"wrong": "นยม", "right": "นิยม"},{"wrong": "หวัน็อก", "right": "หัวน็อก"},
  {"wrong": "คงาม", "right": "ความ"},{"wrong": "คณุธรรม", "right": "คุณธรรม"},{"wrong": "เมื้อ", "right": "เมื่อ"},{"wrong": "หรีอ", "right": "หรือ"},
  {"wrong": "สารมารถ", "right": "สามารถ"},{"wrong": "ข่อดี", "right": "ข้อดี"},{"wrong": "โทรสัพ", "right": "โทรศัพท์"},{"wrong": "ใช้สื้อ", "right": "ใช้สื่อ"},
  {"wrong": "ใข้", "right": "ให้"},{"wrong": "ใข้", "right": "ใช้"},{"wrong": "ก็ตาน", "right": "ก็ตาม"},{"wrong": "ส์งคม", "right": "สังคม"},
  {"wrong": "เส่ยหาย", "right": "เสียหาย"},{"wrong": "ชกจุง", "right": "ชักจูง"},{"wrong": "เชน", "right": "เช่น"},{"wrong": "สือสาร", "right": "สื่อสาร"},
  {"wrong": "ภูมูิคุ้มกัน", "right": "ภูมิคุ้มกัน"},{"wrong": "ถึ่ง", "right": "ถึง"},{"wrong": "ระมัตระวัง", "right": "ระมัดระวัง"},
  {"wrong": "ทิ่", "right": "ที่"},{"wrong": "หลักฐน", "right": "หลักฐาน"},{"wrong": "สรัาง", "right": "สร้าง"},{"wrong": "ควม", "right": "ความ"},
  {"wrong": "สื่ว", "right": "สื่อ"},{"wrong": "ตัวอยาง", "right": "ตัวอย่าง"},{"wrong": "ชันนำ", "right": "ชั้นนำ"},
]


# ----------------------
# แปลงเป็น DataFrame
# ----------------------
df = pd.DataFrame(common_misspellings)

# ----------------------
# บันทึกเป็น CSV
# ----------------------
csv_path = "update_common_misspellings.csv"
df.to_csv(csv_path, index=False, encoding="utf-8-sig")

# ----------------------
# บันทึกเป็น JSON
# ----------------------
json_path = "update_common_misspellings.json"
with open(json_path, "w", encoding="utf-8") as f:
    json.dump(df.to_dict(orient="records"), f, ensure_ascii=False, indent=2)

print(f"บันทึกไฟล์แล้ว:\n- {csv_path}\n- {json_path}")

"""# ดูอันนี้ล่าสุด ได้ 90/100

https://silpakorn-my.sharepoint.com/:x:/g/personal/singtong_c2_su_ac_th/EbsXvf6-ouxGlU4bk386CyYBUUWpDapFZsMywlw3WDKOOg?e=cIF76y
"""

import re
import json
import difflib
import requests
from sentence_transformers import SentenceTransformer, util
from pythainlp.tokenize import word_tokenize
from pythainlp.corpus import thai_words
from pythainlp.tag import pos_tag
from pythainlp.util import normalize
from pythainlp.spell import spell

# ---- ส่วนให้คะแนนใจความ ----

# โหลดโมเดล multilingual ---- paraphrase-multilingual-MiniLM-L12-v2 , airesearch/wangchanberta-base-att-spm-uncased
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
core_sentences = {
    "score_1": "สื่อสังคมหรือสื่อออนไลน์หรือสื่อสังคมออนไลน์เป็นช่องทางที่ใช้ในการเผยแพร่หรือค้นหาหรือรับข้อมูลข่าวสาร",
    "score_2": "การใช้สื่อสังคมหรือสื่อออนไลน์หรือสื่อสังคมออนไลน์อย่างไม่ระมัดระวังหรือขาดความรับผิดชอบจะเกิดโทษหรือผลเสียหรือข้อเสียหรือผลกระทบหรือสิ่งไม่ดี",
    "score_3": "ผู้ใช้ต้องรู้ทันหรือรู้เท่าทันสื่อสังคมออนไลน์",
    "score_4": "การใช้สื่อสังคมหรือสื่อออนไลน์หรือสื่อสังคมออนไลน์ด้วยเจตนาแอบแฝงมีผลกระทบต่อความน่าเชื่อถือของข้อมูลข่าวสาร"
}

def normalize_text(text):
    text = " ".join(text.replace("\n", " ").replace("\r", " ").replace("\t", " ").split())
    return text.replace(" ", "")

def find_keywords_list(text, keywords):
    found = [kw for kw in keywords if kw.replace(" ", "") in text]
    return found

def score_group_1(text):
    text_norm = normalize_text(text)
    media_keywords = ["สื่อสังคมออนไลน์", "สื่อสังคม", "สื่อออนไลน์"]
    usage_keywords = ["เป็นช่องทาง", "ช่องทาง", "เป็นการแพร่กระจาย", "เป็นสื่อ", "สามารถ", "ทำให้", "เป็นการกระจาย", "นั้น"]
    last_keywords = ["แพร่กระจาย", "แพร่กระจายข่าวสาร", "ค้นหา", "รับข้อมูลข่าวสาร", "เผยแพร่", "ติดต่อสื่อสาร", "กระจาย", "รับสาร","รับรู้"]

    found_usage = [kw for kw in usage_keywords if kw.replace(" ", "") in text_norm]
    found_last = [kw for kw in last_keywords if kw.replace(" ", "") in text_norm]
    first_5_words = text.split()[:5]
    first_5_text = "".join(first_5_words)
    found_media_in_first_5 = any(kw in first_5_text for kw in media_keywords)

    score = 1 if (found_media_in_first_5 and found_usage and found_last) else 0
    return score

def score_group_2(text):
    text_norm = normalize_text(text)
    keypoints_1 = ["ไม่ระวัง", "ไม่ระมัดระวัง", "ขาดความรับผิดชอบ", "ควรระมัดระวัง", "ใช้ในทางที่ไม่ดี", "ไม่เหมาะสม", "อย่างระมัดระวัง", "ไตร่ตรอง"]
    keypoints_2 = [
        "โทษ", "ผลเสีย", "ข้อเสีย", "เกิดผลกระทบ", "สิ่งไม่ดี",
        "เสียหาย",
        "การเขียนแสดงความเห็นวิพากษ์วิจารณ์ผู้อื่นในทางเสียหาย",
        "การเขียนแสดงความคิดเห็นวิพากษ์วิจารณ์ผู้อื่นในทางเสียหาย",
        "ตกเป็นเหยื่อของมิจฉาชีพ",
        "ตกเป็นเหยื่อมิจฉาชีพ", "ตกเป็นเหยื่อทางการตลาด"
    ]
    found_1 = find_keywords_list(text_norm, keypoints_1)
    found_2 = find_keywords_list(text_norm, keypoints_2)
    found_illegal = "ผิดกฎหมาย" in text_norm

    score = 1 if (found_1 and found_2) or (found_1 and found_illegal and found_2) else 0
    return score

def score_group_3(text):
    text_norm = normalize_text(text)
    media_keypoint = ["สื่อสังคมออนไลน์", "สื่อสังคม", "สื่อออนไลน์"]
    keypoints = ["รู้เท่าทัน", "รู้ทัน", "ผู้ใช้ต้องรู้เท่าทัน", "รู้ทันสื่อสังคม",
                 "รู้เท่าทันสื่อ", "รู้ทันสื่อ", "สร้างภูมิคุ้มกัน", "ไม่ตกเป็นเหยื่อ", "แก้ปัญหาการตกเป็นเหยื่อ"]

    found_1 = find_keywords_list(text_norm, media_keypoint)
    found_2 = find_keywords_list(text_norm, keypoints)

    score = 1 if (found_1 and found_2) else 0
    return score

def score_group_4(text):
    text_norm = normalize_text(text)
    media_use_keywords = [
        "ใช้สื่อสังคม", "ใช้สื่อออนไลน์", "ใช้สื่อสังคมออนไลน์", "การใช้สื่อ"
    ]
    hidden_intent_keywords = ["เจตนาแอบแฝง"]
    effect_keywords = ["ผลกระทบต่อ", "ผลกระทบ"]
    credibility_keywords = [
        "ความน่าเชื่อถือของข่าวสาร", "ความน่าเชื่อถือของข้อมูลข่าวสาร", "ความน่าเชื่อถือของข้อมูล",
        "มีสติ", "ความน่าเชื่อถือ", "ความเชื่อถือของข้อมูลข่าวสาร", "ข้อมูลข่าวสาร"
    ]
    words = text.split()

    def find_positions(words, keywords):
        positions = []
        joined_text = "".join(words)
        for kw in keywords:
            start = 0
            while True:
                idx = joined_text.find(kw.replace(" ", ""), start)
                if idx == -1:
                    break
                positions.append(len(joined_text[:idx].split()))
                start = idx + len(kw.replace(" ", ""))
        return positions

    media_positions = find_positions(words, media_use_keywords)
    hidden_positions = find_positions(words, hidden_intent_keywords)
    effect_positions = find_positions(words, effect_keywords)
    # ตำแหน่ง media ก่อน hidden หรือ effect (ตามแบบเดิม)
    media_before_hidden = any((0 < h - m <= 5) for m in media_positions for h in hidden_positions)
    media_before_effect = any((0 < e - m <= 5) for m in media_positions for e in effect_positions)

    # ตรวจพบกลุ่ม keyword
    found_hidden_intent = find_keywords_list(text_norm, hidden_intent_keywords)
    found_effect = find_keywords_list(text_norm, effect_keywords)
    found_credibility = find_keywords_list(text_norm, credibility_keywords)

    # ต้องเจอทั้ง hidden_intent ผลกระทบ และ credibility ครบทั้ง 3 อย่าง
    score = 1 if (found_hidden_intent and found_effect and found_credibility) else 0
    return score

def evaluate_mind_score(answer_text):
    score1 = score_group_1(answer_text)
    score2 = score_group_2(answer_text)
    score3 = score_group_3(answer_text)
    score4 = score_group_4(answer_text)
    total_score = score1 + score2 + score3 + score4

    result = {
        "ใจความที่ 1": score1,
        "ใจความที่ 2": score2,
        "ใจความที่ 3": score3,
        "ใจความที่ 4": score4,
        "คะแนนรวมใจความ ": total_score
    }

    return result

# ---- ส่วนให้คะแนนการสะกดคำ ----

# โหลด whitelist คำทับศัพท์
with open('/content/drive/MyDrive/thai_loanwords_new_update.json', 'r', encoding='utf-8') as f:
    loanwords_data = json.load(f)
    loanwords_whitelist = set(item['thai_word'] for item in loanwords_data)

# โหลด common misspellings JSON
with open('/content/drive/MyDrive/update_common_misspellings.json', 'r', encoding='utf-8') as f:
    data = json.load(f)
    COMMON_MISSPELLINGS = {item['wrong']: item['right'] for item in data}

API_KEY = '33586c7cf5bfa0029887a9831bf94963' # add Apikey
API_URL = 'https://api.longdo.com/spell-checker/proof'

custom_words = {"ประเทศไทย", "สถาบันการศึกษา", "นานาประการ"}

#คำที่สามารถฉีกคำได้
allowed_splitable_phrases = {
    'แม้ว่า', 'ถ้าแม้ว่า', 'แต่ถ้า', 'แต่ทว่า', 'เนื่องจาก', 'ดังนั้น', 'เพราะฉะนั้น','ตกเป็น','เป็นการ','มีประโยชน์','ทำได้','ก็ตาม','แก้ปัญหา','มีผลกระทบ','ควรระวัง','รู้เท่าทัน','โฆษณาชวนเชื่อ',
    'ดีแต่', 'หรือไม่', 'ข้อมูลข่าวสาร', 'ทั่วโลก', 'ยังมี', 'ทำให้เกิด', 'เป็นโทษ', 'ไม่มี', 'ข้อควรระวัง', 'การแสดงความคิดเห็น', 'ผิดกฎหมาย', 'แสดงความคิดเห็น',
    'เป็นประโยชน์','ช่องทาง','เสียหาย'
}
#คำที่ไม่สามารถฉีกคำได้
strict_not_split_words = {
    'มากมาย', 'ประเทศไทย', 'ออนไลน์', 'ความคิดเห็น', 'ความน่าเชื่อถือ'
}

thai_dict = set(w for w in set(thai_words()).union(custom_words) if (' ' not in w) and w.strip())

# allowed punctuation (เพิ่ม ' และ ")
allowed_punctuations = {'.', ',', '-', '(', ')', '!', '?', '%', '“', '”', '‘', '’', '"', "'", '…', 'ฯ'}

# Allow / Forbid list ไม้ยมก (เพิ่มคำที่ใช้บ่อย)
allow_list = {'ปี', 'อื่น', 'เล็ก', 'ใหญ่', 'มาก', 'หลาย', 'ช้า', 'เร็ว', 'ชัด', 'ดี', 'ผิด' ,'เสีย', 'หาย'}
forbid_list = {'นา', 'บางคน', 'บางอย่าง', 'บางสิ่ง', 'บางกรณี'}

#------------------------------------------------------------------------#

# ตรวจการฉีกคำ
def check_linebreak_issue(prev_line_tokens, next_line_tokens, max_words=3):
    last_word = prev_line_tokens[-1]
    first_word = next_line_tokens[0]
    if last_word.endswith('-') or first_word.startswith('-'):
        return False, None, None, None
    for prev_n in range(1, min(max_words, len(prev_line_tokens)) + 1):
        prev_part = ''.join(prev_line_tokens[-prev_n:])
        for next_n in range(1, min(max_words, len(next_line_tokens)) + 1):
            next_part = ''.join(next_line_tokens[:next_n])
            combined = normalize(prev_part + next_part)
            if (
                (' ' not in combined)
                and (combined not in allowed_splitable_phrases)  # ✅ ข้ามถ้าเป็นวลีที่อนุญาต
                and (
                    (combined in strict_not_split_words) or (
                        (combined in thai_dict)
                        and (len(word_tokenize(combined, engine='newmm')) == 1)
                    )
                )
            ):
                return True, prev_part, next_part, combined
    return False, None, None, None

# วนตรวจทั้งข้อความทีละบรรทัด
def analyze_linebreak_issues(text):
    lines = text.strip().splitlines()
    issues = []
    for i in range(len(lines) - 1):
        prev_line = lines[i].strip()
        next_line = lines[i + 1].strip()
        prev_tokens = word_tokenize(prev_line)
        next_tokens = word_tokenize(next_line)
        if not prev_tokens or not next_tokens:
            continue
        issue, prev_part, next_part, combined = check_linebreak_issue(prev_tokens, next_tokens)
        if issue:
            issues.append({
                'line_before': prev_line,
                'line_after': next_line,
                'prev_part': prev_part,
                'next_part': next_part,
                'combined': combined,
                'pos_in_text': (i, len(prev_tokens))
            })
    return issues

# รวมข้อความหรือคำที่ถูกตัดข้ามบรรทัด
def merge_linebreak_words(text, linebreak_issues):
    lines = text.splitlines()
    for issue in reversed(linebreak_issues):
        i, _ = issue['pos_in_text']
        lines[i] = lines[i].rstrip() + issue['combined'] + lines[i+1].lstrip()[len(issue['next_part']):]
        lines.pop(i+1)
    return "\n".join(lines)

#ข้ามอังกฤษและตัวเลข
def is_english_or_number(word: str) -> bool:
    """
    คืน True ถ้า word เป็นภาษาอังกฤษหรือตัวเลข
    """
    w = word.strip()
    # เช็คถ้าเป็นตัวอักษร A-Z, a-z, ตัวเลข 0-9 หรือมีแต่พวก .,()-_/ ปน
    return bool(re.fullmatch(r"[A-Za-z0-9().,\-_/]+", w))

# 1. ตรวจการสะกดคำด้วย PyThaiNLP + Longdo
def pythainlp_spellcheck(tokens, pos_tags, dict_words, ignore_words):
    """ ตรวจสอบการสะกดคำด้วย PyThaiNLP คืน list ของ dict ที่มี 'word', 'pos', 'index' """
    mistakes = []
    for i, token in enumerate(tokens):
        # Skip empty tokens, numbers, and English words
        if not token or is_english_or_number(token):
            continue

        # Check if the word is in our custom dictionary
        if token in ignore_words:
            continue

        # Check if the word is in the general Thai dictionary
        if token in dict_words:
            continue
        # Use PyThaiNLP spell checker
        suggestions = spell(token)

        # If no suggestions and not in dictionaries, consider it a potential error
        if not suggestions:
            mistakes.append({
                             'word': token,
                             'pos': pos_tags[i][1] if i < len(pos_tags) else None,
                             'index': i, 'suggestions': suggestions })
    return mistakes

# -------------------------------
def spellcheck_before_tokenize(text):
    words = re.findall(r'[ก-๙]+', text)  # ✅ ดึงเฉพาะคำไทย
    pos_tags = pos_tag(words, corpus='orchid')

    # filter ข้ามอังกฤษ/เลข อีกชั้น
    words = [w for w in words if not is_english_or_number(w)]

    pythai_errors = pythainlp_spellcheck(
        words, pos_tags,
        dict_words=thai_dict,
        ignore_words=custom_words
    )
    wrong_words = [e['word'] for e in pythai_errors]

    longdo_results = longdo_spellcheck_batch(wrong_words)
    spelling_errors_legit = [
        {**e, 'suggestions': longdo_results.get(e['word'], [])}
        for e in pythai_errors if e['word'] in longdo_results
    ]
    return spelling_errors_legit

#longdo spell checker
def longdo_spellcheck_batch(words):
    """
    ตรวจสอบคำผิดด้วย Longdo API แบบ batch
    คืน dict {word: [suggestions]}
    """
    results = {}
    headers = {"Content-Type": "application/json"}
    for word in words:
        try:
            payload = {
                "text": word,
                "api": API_KEY
            }
            response = requests.post(API_URL, headers=headers, data=json.dumps(payload))
            if response.status_code == 200:
                data = response.json()
                suggestions = []
                # Longdo จะส่งผลลัพธ์เป็น list ของคำ
                if "words" in data and data["words"]:
                    for w in data["words"]:
                        if "candidates" in w:
                            suggestions.extend(c["text"] for c in w["candidates"])
                results[word] = suggestions
            else:
                results[word] = []
        except Exception as e:
            results[word] = []
    return results

# ตรวจ common misspellings ก่อน tokenize (จากข้อความดิบ)
def check_common_misspellings_before_tokenize(text, misspelling_dict):
    """
    text : ข้อความดิบ (ยังไม่ tokenize)
    misspelling_dict : dict จากไฟล์ JSON
    คืน list ของ dict ที่มี 'word', 'index', 'right'
    """
    errors = []
    for wrong, right in misspelling_dict.items():
        if wrong in text:
            for m in re.finditer(re.escape(wrong), text):
                errors.append({
                    "word": wrong,
                    "index": m.start(),   # ตำแหน่งในข้อความดิบ
                    "right": right
                })
    return errors

# ตรวจ loanwords ก่อน tokenize
def check_loanword_before_tokenize(words, whitelist):
    mistakes = []
    for i, w in enumerate(words):
        if is_english_or_number(w):  # ข้ามอังกฤษ+ตัวเลข
            continue
        matches = difflib.get_close_matches(w, list(whitelist), n=1, cutoff=0.7)
        if matches and w not in whitelist:
            mistakes.append({
                'word': w,
                'index': i,
                'suggestions': [str(matches[0])] if matches else [] # Ensure suggestions is a list of strings
            })
    return mistakes


#ตรวจการใช้เครื่องหมายที่ไม่อนุญาต
def find_unallowed_punctuations(text):
    pattern = f"[^{''.join(re.escape(p) for p in allowed_punctuations)}a-zA-Z0-9ก-๙\\s]"
    return set(re.findall(pattern, text))

#ใช้แยกไม้ยมกออกจากคำที่ติดกัน
def separate_maiyamok(text):
    return re.sub(r'(\S+?)ๆ', r'\1 ๆ', text)
#ตรวจการใช้ไม้ยมก
def analyze_maiyamok(tokens, pos_tags):
    results = []
    found_invalid = False
    VALID_POS = {'NCMN', 'NNP', 'VACT', 'VNIR', 'CLFV', 'ADVN', 'ADVI', 'ADVP', 'PRP', 'ADV'}
    for i, token in enumerate(tokens):
        if token == 'ๆ':
            prev_idx = i - 1
            prev_word = tokens[prev_idx] if prev_idx >= 0 else None
            prev_tag = pos_tags[prev_idx][1] if prev_idx >= 0 else None
            if prev_word is None or prev_word == 'ๆ':
                verdict = "❌ ไม้ยมกไม่ควรขึ้นต้นประโยค/คำ"
            elif prev_word in forbid_list:
                verdict = '❌ ไม่ควรใช้ไม้ยมกกับคำนี้'
            elif (prev_tag in VALID_POS) or (prev_word in allow_list):
                verdict = '✅ ถูกต้อง (ใช้ไม้ยมกซ้ำคำได้)'
            else:
                verdict = '❌ ไม่ควรใช้ไม้ยมok นอกจากกับคำนาม/กริยา/วิเศษณ์'
            context = tokens[max(0, i-2):min(len(tokens), i+3)]
            results.append({
                'คำก่อนไม้ยมก': prev_word or '',
                'POS คำก่อน': prev_tag or '',
                'บริบท': ' '.join(context),
                'สถานะ': verdict
            })
            if verdict.startswith('❌'):
                found_invalid = True
    return results, found_invalid

#ตรวจการแยกคำ
def detect_split_errors(tokens, custom_words=None):
    check_dict = set(thai_words()).union(custom_words or [])
    check_dict = {w for w in check_dict if (' ' not in w) and w.strip()}
    errors = []
    for i in range(len(tokens) - 1):
        combined = tokens[i] + tokens[i + 1]
        if (' ' not in combined) and (combined in check_dict) and (combined not in splitable_phrases):
            errors.append({
                "split_pair": (tokens[i], tokens[i+1]),
                "suggested": combined
            })
    return errors

#วิเคราะห์เงื่อนไข
def evaluate_text(text):
    # -----------------------------
    # จัดการตัดบรรทัด
    # -----------------------------
    linebreak_issues = analyze_linebreak_issues(text)
    corrected_text = merge_linebreak_words(text, linebreak_issues)

    # tokenize
    tokens = [t for t in word_tokenize(corrected_text, engine='newmm', keep_whitespace=False)
          if not is_english_or_number(t)]
    pos_tags = pos_tag(tokens, corpus='orchid')

    # ตรวจ spelling ด้วย PyThaiNLP
    pythai_errors = pythainlp_spellcheck(tokens, pos_tags, dict_words=thai_dict, ignore_words=custom_words)

    # ✅ ตรวจ common misspellings จากข้อความดิบ (ไม่ใช้ tokens)
    json_misspells = check_common_misspellings_before_tokenize(corrected_text, COMMON_MISSPELLINGS)

    # ตรวจ loanwords
    loanword_errors = check_loanword_before_tokenize(tokens, loanwords_whitelist)

    # ตรวจ Longdo หลังจาก tokenize
    longdo_results = longdo_spellcheck_batch([e['word'] for e in pythai_errors])
    longdo_errors = [
        {**e, 'suggestions': longdo_results.get(e['word'], [])}
        for e in pythai_errors
    ]

    # รวม spelling errors ทั้งหมด
    all_spelling_errors = longdo_errors + [
        {
            "word": e["word"],
            "pos": None,
            "index": e["index"],
            "suggestions": [e["right"]],
        }
        for e in json_misspells
    ] + loanword_errors

    # ตรวจ punctuation, maiyamok, split word เหมือนเดิม
    punct_errors = find_unallowed_punctuations(corrected_text)
    maiyamok_results, has_wrong_maiyamok = analyze_maiyamok(tokens, pos_tags)
    split_errors = detect_split_errors(tokens, custom_words=custom_words)

    # รวมผลและให้คะแนนเหมือนเดิม
    error_counts = {
        "spelling": len(all_spelling_errors),
        "linebreak": len(linebreak_issues),
        "split": len(split_errors),
        "punct": len(punct_errors),
        "maiyamok": sum(1 for r in maiyamok_results if r['สถานะ'].startswith('❌'))
    }

    # สร้าง reasons
    reasons = []
    if error_counts["linebreak"]:
        details = [f"{issue['prev_part']} + {issue['next_part']} → {issue['combined']}" for issue in linebreak_issues]
        reasons.append("พบการฉีกคำข้ามบรรทัด: " + "; ".join(details))
    if error_counts["split"]:
        details = [f"{e['split_pair'][0]} + {e['split_pair'][1]} → {e['suggested']}" for e in split_errors]
        reasons.append("พบการแยกคำผิด: " + "; ".join(details))
    if error_counts["spelling"]:
        error_words = [f"{e['word']} (แนะนำ: {', '.join(map(str, e.get('suggestions', [])))})" for e in all_spelling_errors]
        reasons.append(f"ตรวจเจอคำสะกดผิดหรือทับศัพท์ผิด: {', '.join(error_words)}")
    if error_counts["punct"]:
        reasons.append(f"ใช้เครื่องหมายที่ไม่อนุญาต: {', '.join(punct_errors)}")
    if error_counts["maiyamok"]:
        wrong_desc = [x for x in maiyamok_results if x['สถานะ'].startswith('❌')]
        texts = [f"{x['คำก่อนไม้ยมก']}: {x['สถานะ']}" for x in wrong_desc]
        reasons.append("ใช้ไม้ยมกผิด: " + '; '.join(texts))
    if not reasons:
        reasons.append("ไม่มีปัญหา")

    # คะแนนสะกดคำ
    if sum(error_counts.values()) == 0:
        score = 1.0
    elif sum(c > 0 for c in error_counts.values()) == 1 and max(error_counts.values()) >= 2:
        score = 0.0
    elif sum(c > 0 for c in error_counts.values()) == 1:
        score = 0.5
    else:
        score = 0.0

    return {
        'linebreak_issues': linebreak_issues,
        'spelling_errors': all_spelling_errors,
        'loanword_spell_errors': loanword_errors,
        'punctuation_errors': list(punct_errors),
        'maiyamok_results': maiyamok_results,
        'split_errors': split_errors,
        'reasons': reasons,
        'score': score
    }

def evaluate_single_answer(answer_text):

    # --- ตรวจความใกล้เคียงด้วย cosine similarity ---
    student_emb = model.encode(answer_text, convert_to_tensor=True)
    core_embs = model.encode(list(core_sentences.values()), convert_to_tensor=True)
    cosine_scores = util.cos_sim(student_emb, core_embs)[0]
    best_score = cosine_scores.max().item()

    # ถ้า similarity < 0.6 → ไม่ให้คิดคะแนนใจความสำคัญเลย
    if best_score < 0.6:
        result = {
            "cosine_similarity": best_score,
            "คะแนนใจความสำคัญ": {"ใจความที่ 1": 0, "ใจความที่ 2": 0,
                                "ใจความที่ 3": 0, "ใจความที่ 4": 0,
                                "คะแนนรวมใจความ ": 0},
            "คะแนนการสะกดคำ": 0.0,
            "เงื่อนไขที่ผิด": ["Cosine similarity < 0.6 ไม่ตรวจใจความสำคัญ"],
            "คะแนนรวมทั้งหมด": 0.0
        }
        return json.dumps(result, ensure_ascii=False, indent=2)


    # คะแนนใจความ
    # --- ถ้า similarity ≥ 0.6 → คำนวณใจความตามปกติ ---
    mind_score = evaluate_mind_score(answer_text)
    mind_total = mind_score["คะแนนรวมใจความ "]

    if mind_total == 0:
        spelling_score = 0.0
        combined_score = 0.0
        spelling_reason = ["ไม่ได้คะแนนใจความสำคัญ (ใจความ = 0) จึงไม่ตรวจการสะกดคำ"]
    else:
        # คะแนนการสะกดคำ
        res = evaluate_text(str(answer_text))
        spelling_score = res["score"]   # ได้เป็น 0.0 / 0.5 / 1.0

        # รวมคะแนน (ถ่วงน้ำหนักหรือบวกตามที่ต้องการ)
        combined_score = mind_total + spelling_score
        spelling_reason = res["reasons"]

    result = {
        "cosine_similarity": best_score,
        "คะแนนใจความสำคัญ (4 คะแนน)": mind_score,
        "คะแนนการสะกดคำ (1 คะแนน)": spelling_score,
        "เงื่อนไขที่ผิด": spelling_reason,
        "คะแนนรวมทั้งหมด": combined_score
    }
    return json.dumps(result, ensure_ascii=False, indent=2)


#-------CSV---------
import pandas as pd
import json

# --- โหลดไฟล์คำตอบ ---
# โหลด dataset เดิม
df = pd.read_csv("/content/drive/MyDrive/dataset_S4(ใหม่)(Sheet1).csv")

# เตรียม list สำหรับเก็บผล
คะแนนสะกด_list = []
เงื่อนไขผิด_list = []

for idx, row in df.iterrows():
    answer_text = str(row["student_answer_1"]) if pd.notna(row["student_answer_1"]) else ""
    result_json = evaluate_single_answer(answer_text)
    result = json.loads(result_json)

    # ดึงค่าใจความ
    mind_score = result.get("คะแนนใจความสำคัญ (4 คะแนน)", {})
    คะแนนสะกด_list.append(result.get("คะแนนการสะกดคำ (1 คะแนน)", 0))
    เงื่อนไขผิด_list.append("; ".join(result.get("เงื่อนไขที่ผิด", [])))

# เพิ่มคอลัมน์ใหม่เข้า DataFrame เดิม
df["S4"] = คะแนนสะกด_list
df["เงื่อนไขที่ผิด"] = เงื่อนไขผิด_list

# เซฟทับไฟล์เดิม (หรือเปลี่ยนชื่อถ้าอยากเก็บต้นฉบับไว้)
df.to_csv("/content/sample_data/new27_score_s4.csv", index=False, encoding="utf-8-sig")

print("✅ ประเมินเสร็จแล้ว คอลัมน์ใน new_dataset.csv")
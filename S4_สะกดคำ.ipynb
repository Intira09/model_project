{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from pythainlp.corpus import thai_words\n",
        "from pythainlp.tag import pos_tag\n",
        "from pythainlp.util import normalize\n",
        "import json\n",
        "import difflib\n",
        "\n",
        "# โหลด whitelist (แก้ path ตามจริง หรือใช้ set() ถ้าไม่มีไฟล์)\n",
        "try:\n",
        "    with open('thai_loanwords.json', 'r', encoding='utf-8') as f:\n",
        "        loanwords_whitelist = set(json.load(f))\n",
        "except:\n",
        "    loanwords_whitelist = set()\n",
        "\n",
        "API_KEY = ' ' # add Apikey\n",
        "API_URL = 'https://api.longdo.com/spell-checker/proof'\n",
        "\n",
        "custom_words = {\"ประเทศไทย\", \"สถาบันการศึกษา\", \"นานาประการ\"}\n",
        "splitable_phrases = {\n",
        "    'แม้ว่า', 'ถ้าแม้ว่า', 'แต่ถ้า', 'แต่ทว่า', 'เนื่องจาก', 'ดังนั้น', 'เพราะฉะนั้น','ตกเป็น',\n",
        "    'ดีแต่', 'หรือไม่', 'ข้อมูลข่าวสาร', 'ทั่วโลก', 'ยังมี', 'ทำให้เกิด', 'เป็นโทษ', 'ไม่มี', 'ข้อควรระวัง', 'การแสดงความคิดเห็น', 'ผิดกฎหมาย', 'แสดงความคิดเห็น'\n",
        "}\n",
        "strict_not_split_words = {\n",
        "    'มากมาย', 'ประเทศไทย', 'ออนไลน์', 'ความคิดเห็น', 'ความน่าเชื่อถือ'\n",
        "}\n",
        "\n",
        "thai_dict = set(w for w in set(thai_words()).union(custom_words) if (' ' not in w) and w.strip())\n",
        "\n",
        "# allowed punctuation (เพิ่ม ' และ \")\n",
        "allowed_punctuations = {'.', ',', '-', '(', ')', '!', '?', '%', '“', '”', '‘', '’', '\"', \"'\", '…', 'ฯ'}\n",
        "\n",
        "# Allow / Forbid list ไม้ยมก (เพิ่มคำที่ใช้บ่อย)\n",
        "allow_list = {'ปี', 'อื่น', 'เล็ก', 'ใหญ่', 'มาก', 'หลาย', 'ช้า', 'เร็ว', 'ชัด', 'ดี', 'ผิด'}\n",
        "forbid_list = {'นา', 'บางคน', 'บางอย่าง', 'บางสิ่ง', 'บางกรณี'}\n",
        "\n",
        "explanations = [\n",
        "    \"1. ตรวจสอบการตัดคำผิดขณะขึ้นบรรทัดใหม่ (ถ้ามี)\",\n",
        "    \"2. ตรวจสอบคำสะกดผิดด้วย PyThaiNLP (และขอ Longdo ช่วยกรณีสงสัย)\",\n",
        "    \"3. ตรวจสอบการใช้เครื่องหมายวรรคตอนที่ไม่อนุญาต\",\n",
        "    \"4. ตรวจสอบการใช้ไม้ยมก (ๆ) ถูกต้องตามบริบทหรือไม่\",\n",
        "    \"5. ตรวจสอบการแยกคำผิด เช่น คำที่ควรติดกัน\"\n",
        "]\n",
        "\n",
        "#------------------------------------------------------------------------#\n",
        "def check_linebreak_issue(prev_line_tokens, next_line_tokens, max_words=3):\n",
        "    last_word = prev_line_tokens[-1]\n",
        "    first_word = next_line_tokens[0]\n",
        "    if last_word.endswith('-') or first_word.startswith('-'):\n",
        "        return False, None, None, None\n",
        "    for prev_n in range(1, min(max_words, len(prev_line_tokens)) + 1):\n",
        "        prev_part = ''.join(prev_line_tokens[-prev_n:])\n",
        "        for next_n in range(1, min(max_words, len(next_line_tokens)) + 1):\n",
        "            next_part = ''.join(next_line_tokens[:next_n])\n",
        "            combined = normalize(prev_part + next_part)\n",
        "            if (\n",
        "                (' ' not in combined)\n",
        "                and (combined not in splitable_phrases)\n",
        "                and (\n",
        "                    (combined in strict_not_split_words) or (\n",
        "                        (combined in thai_dict)\n",
        "                        and (len(word_tokenize(combined, engine='newmm')) == 1)\n",
        "                    )\n",
        "                )\n",
        "            ):\n",
        "                return True, prev_part, next_part, combined\n",
        "    return False, None, None, None\n",
        "\n",
        "def analyze_linebreak_issues(text):\n",
        "    lines = text.strip().splitlines()\n",
        "    issues = []\n",
        "    for i in range(len(lines) - 1):\n",
        "        prev_line = lines[i].strip()\n",
        "        next_line = lines[i + 1].strip()\n",
        "        prev_tokens = word_tokenize(prev_line)\n",
        "        next_tokens = word_tokenize(next_line)\n",
        "        if not prev_tokens or not next_tokens:\n",
        "            continue\n",
        "        issue, prev_part, next_part, combined = check_linebreak_issue(prev_tokens, next_tokens)\n",
        "        if issue:\n",
        "            issues.append({\n",
        "                'line_before': prev_line,\n",
        "                'line_after': next_line,\n",
        "                'prev_part': prev_part,\n",
        "                'next_part': next_part,\n",
        "                'combined': combined,\n",
        "                'pos_in_text': (i, len(prev_tokens))\n",
        "            })\n",
        "    return issues\n",
        "\n",
        "def merge_linebreak_words(text, linebreak_issues):\n",
        "    lines = text.splitlines()\n",
        "    for issue in reversed(linebreak_issues):\n",
        "        i, _ = issue['pos_in_text']\n",
        "        lines[i] = lines[i].rstrip() + issue['combined'] + lines[i+1].lstrip()[len(issue['next_part']):]\n",
        "        lines.pop(i+1)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def pythainlp_spellcheck(tokens, pos_tags, dict_words=None, ignore_words=None):\n",
        "    if dict_words is None:\n",
        "        dict_words = thai_dict\n",
        "    if ignore_words is None:\n",
        "        ignore_words = set()\n",
        "    misspelled = []\n",
        "    for i, w in enumerate(tokens):\n",
        "        if not w.strip() or w in dict_words or w in ignore_words or len(w) == 1 or 'ๆ' in w:\n",
        "            continue\n",
        "        misspelled.append({\n",
        "            'word': w,\n",
        "            'pos': pos_tags[i][1] if i < len(pos_tags) else None,\n",
        "            'index': i\n",
        "        })\n",
        "    return misspelled\n",
        "\n",
        "def longdo_spellcheck_batch(words):\n",
        "    results = {}\n",
        "    if not words:\n",
        "        return results\n",
        "    try:\n",
        "        headers = {'Content-Type': 'application/json'}\n",
        "        payload = {\"key\": API_KEY, \"text\": \"\\n\".join(words)}\n",
        "        response = requests.post(API_URL, headers=headers, json=payload, timeout=6)\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            for e in result.get(\"result\", []):\n",
        "                if e.get(\"suggestions\"):\n",
        "                    results[e[\"word\"]] = e[\"suggestions\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Exception calling longdo: {e}\")\n",
        "    return results\n",
        "\n",
        "def check_loanword_spelling(tokens, whitelist):\n",
        "    mistakes = []\n",
        "    for tok in tokens:\n",
        "        # Find close matches with a lower cutoff for loanwords\n",
        "        matches = difflib.get_close_matches(tok, list(whitelist), n=1, cutoff=0.7) # Lowered cutoff\n",
        "        if matches and tok not in whitelist:\n",
        "            mistakes.append({'found': tok, 'should_be': matches[0]})\n",
        "    return mistakes\n",
        "\n",
        "\n",
        "def find_unallowed_punctuations(text):\n",
        "    pattern = f\"[^{''.join(re.escape(p) for p in allowed_punctuations)}a-zA-Z0-9ก-๙\\\\s]\"\n",
        "    return set(re.findall(pattern, text))\n",
        "\n",
        "def separate_maiyamok(text):\n",
        "    return re.sub(r'(\\S+?)ๆ', r'\\1 ๆ', text)\n",
        "\n",
        "def analyze_maiyamok(tokens, pos_tags):\n",
        "    results = []\n",
        "    found_invalid = False\n",
        "    VALID_POS = {'NCMN', 'NNP', 'VACT', 'VNIR', 'CLFV', 'ADVN', 'ADVI', 'ADVP', 'PRP', 'ADV'}\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token == 'ๆ':\n",
        "            prev_idx = i - 1\n",
        "            prev_word = tokens[prev_idx] if prev_idx >= 0 else None\n",
        "            prev_tag = pos_tags[prev_idx][1] if prev_idx >= 0 else None\n",
        "            if prev_word is None or prev_word == 'ๆ':\n",
        "                verdict = \"❌ ไม้ยมกไม่ควรขึ้นต้นประโยค/คำ\"\n",
        "            elif prev_word in forbid_list:\n",
        "                verdict = '❌ ไม่ควรใช้ไม้ยมกกับคำนี้'\n",
        "            elif (prev_tag in VALID_POS) or (prev_word in allow_list):\n",
        "                verdict = '✅ ถูกต้อง (ใช้ไม้ยมกซ้ำคำได้)'\n",
        "            else:\n",
        "                verdict = '❌ ไม่ควรใช้ไม้ยมok นอกจากกับคำนาม/กริยา/วิเศษณ์'\n",
        "            context = tokens[max(0, i-2):min(len(tokens), i+3)]\n",
        "            results.append({\n",
        "                'คำก่อนไม้ยมก': prev_word or '',\n",
        "                'POS คำก่อน': prev_tag or '',\n",
        "                'บริบท': ' '.join(context),\n",
        "                'สถานะ': verdict\n",
        "            })\n",
        "            if verdict.startswith('❌'):\n",
        "                found_invalid = True\n",
        "    return results, found_invalid\n",
        "\n",
        "def detect_split_errors(tokens, custom_words=None):\n",
        "    check_dict = set(thai_words()).union(custom_words or [])\n",
        "    check_dict = {w for w in check_dict if (' ' not in w) and w.strip()}\n",
        "    errors = []\n",
        "    for i in range(len(tokens) - 1):\n",
        "        combined = tokens[i] + tokens[i + 1]\n",
        "        if (' ' not in combined) and (combined in check_dict) and (combined not in splitable_phrases):\n",
        "            errors.append({\n",
        "                \"split_pair\": (tokens[i], tokens[i+1]),\n",
        "                \"suggested\": combined\n",
        "            })\n",
        "    return errors\n",
        "\n",
        "def evaluate_text(text):\n",
        "    # วิเคราะห์\n",
        "    linebreak_issues = analyze_linebreak_issues(text)\n",
        "    corrected_text = merge_linebreak_words(text, linebreak_issues)\n",
        "    tokens = word_tokenize(corrected_text, engine='newmm', keep_whitespace=False)\n",
        "    pos_tags = pos_tag(tokens, corpus='orchid')\n",
        "\n",
        "    # ตรวจคำทับศัพท์\n",
        "    loanword_spell_errors = check_loanword_spelling(tokens, loanwords_whitelist)\n",
        "\n",
        "    # ตรวจสะกด\n",
        "    pythai_errors = pythainlp_spellcheck(tokens, pos_tags, dict_words=thai_dict, ignore_words=custom_words)\n",
        "    wrong_words = [e['word'] for e in pythai_errors]\n",
        "    longdo_results = longdo_spellcheck_batch(wrong_words)\n",
        "    spelling_errors_legit = [\n",
        "        {**e, 'suggestions': longdo_results.get(e['word'], [])}\n",
        "        for e in pythai_errors if e['word'] in longdo_results\n",
        "    ]\n",
        "\n",
        "    # อื่น ๆ\n",
        "    punct_errors = find_unallowed_punctuations(text)\n",
        "    maiyamok_results, has_wrong_maiyamok = analyze_maiyamok(tokens, pos_tags)\n",
        "    split_errors = detect_split_errors(tokens, custom_words=custom_words)\n",
        "\n",
        "    # ==== นับจำนวนข้อผิดพลาดแต่ละประเภท ====\n",
        "    error_counts = {\n",
        "        \"spelling\": len(spelling_errors_legit) + len(loanword_spell_errors),\n",
        "        \"linebreak\": len(linebreak_issues),\n",
        "        \"split\": len(split_errors),\n",
        "        \"punct\": len(punct_errors),\n",
        "        \"maiyamok\": sum(1 for r in maiyamok_results if r['สถานะ'].startswith('❌'))\n",
        "    }\n",
        "    n_issue_types = sum(1 for c in error_counts.values() if c > 0)\n",
        "    multi_in_single_type = any(c >= 2 for c in error_counts.values())\n",
        "\n",
        "    # ==== สร้าง reasons ====\n",
        "    reasons = []\n",
        "    if error_counts[\"linebreak\"]:\n",
        "        details = [f\"{issue['prev_part']} + {issue['next_part']} → {issue['combined']}\" for issue in linebreak_issues]\n",
        "        reasons.append(\"พบการฉีกคำข้ามบรรทัด: \" + \"; \".join(details))\n",
        "    if error_counts[\"split\"]:\n",
        "        details = [f\"{e['split_pair'][0]} + {e['split_pair'][1]} → {e['suggested']}\" for e in split_errors]\n",
        "        reasons.append(\"พบการแยกคำผิด: \" + \"; \".join(details))\n",
        "    if error_counts[\"spelling\"]:\n",
        "        error_words = [e['word'] for e in spelling_errors_legit]\n",
        "        error_desc = [f\"{e['found']} (ควรเป็น {e['should_be']})\" for e in loanword_spell_errors]\n",
        "        reasons.append(f\"ตรวจเจอคำสะกดผิดหรือทับศัพท์ผิด: {', '.join(error_words + error_desc)}\")\n",
        "    if error_counts[\"punct\"]:\n",
        "        reasons.append(f\"ใช้เครื่องหมายที่ไม่อนุญาต: {', '.join(punct_errors)}\")\n",
        "    if error_counts[\"maiyamok\"]:\n",
        "        wrong_desc = [x for x in maiyamok_results if x['สถานะ'].startswith('❌')]\n",
        "        texts = [f\"{x['คำก่อนไม้ยมก']}: {x['สถานะ']}\" for x in wrong_desc]\n",
        "        reasons.append(\"ใช้ไม้ยมกผิด: \" + '; '.join(texts))\n",
        "    if not reasons:\n",
        "        reasons.append(\"ไม่มีปัญหา\")\n",
        "\n",
        "    # ==== เกณฑ์การให้คะแนน ====\n",
        "    if sum(error_counts.values()) == 0:\n",
        "        score = 1.0\n",
        "    elif n_issue_types == 1 and multi_in_single_type:\n",
        "        score = 0.0\n",
        "    elif n_issue_types == 1:\n",
        "        score = 0.5\n",
        "    else:\n",
        "        score = 0.0\n",
        "\n",
        "    return {\n",
        "        'score': score,\n",
        "        'linebreak_issues': linebreak_issues,\n",
        "        'spelling_errors': spelling_errors_legit,\n",
        "        'loanword_spell_errors': loanword_spell_errors,\n",
        "        'punctuation_errors': list(punct_errors),\n",
        "        'maiyamok_results': maiyamok_results,\n",
        "        'split_errors': split_errors,\n",
        "        'reasons': reasons,\n",
        "        'explanations': explanations\n",
        "    }\n",
        "\n",
        "def evaluate_single_answer(answer_text):\n",
        "    res = evaluate_text(str(answer_text))\n",
        "    # เลือกเฉพาะ field ที่ต้องการออกมาเป็น JSON\n",
        "    result = {\n",
        "        \"score\": res[\"score\"],\n",
        "        \"reasons\": res[\"reasons\"]\n",
        "    }\n",
        "    return json.dumps(result, ensure_ascii=False, indent=2)\n",
        "\n",
        "# --- ตัวอย่างการใช้งาน ---\n",
        "answer = \"\"\"สื่อสังคมออนไลน์ นั้น เป็นสื่อช่องทางที่แพร่กระจายข้อมูลข่าวสารในรูปแบบต่างๆหาก\n",
        "ใช้สื่อสังคมไม่ระมัดระวัง ไม่ว่าจะเป็นการเขียนแสดงความคิดเห็นวิพากษ์วิจารณ์ผู้อื่น\n",
        "ในทางเสียหายผิดกฎหมาย เป็นต้น ปัจจุบันผู้คนจำนวนไม่น้อยใช้สื่อสังคมออนไลน์\n",
        "เป็นช่องทางในการทำการตลาดทั้งในทางธุรกิจ สังคม และการเมืองจึงได้ผลดีแบบก้าวกระ\n",
        "โดด แม้ว่าจะมีการใช้สื่อสังคมออนไลน์ในทางสร้างสรรค์ให้แก่สังคมการส่งข่าวสารช่วยเหลือเป็นต้น\"\"\"\n",
        "\n",
        "print(evaluate_single_answer(answer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CWDgR055-Uui",
        "outputId": "30b11975-4a9d-41db-ba72-3ed49097081e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"score\": 0.5,\n",
            "  \"reasons\": [\n",
            "    \"พบการฉีกคำข้ามบรรทัด: กระ + โดด → กระโดด\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}